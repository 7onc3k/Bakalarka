The Architecture of Agency: An Empirical Analysis of Scaffolding Instructions on AI Coding Agent PerformanceThe transition from intelligent code autocompletion to autonomous software engineering agents marks a fundamental shift in software development, frequently categorized in the contemporary literature as Software Engineering 3.0. In this emerging paradigm, large language model (LLM) agents do not merely suggest discrete lines of code; they actively perceive project states, formulate multi-step plans, execute complex toolchains, and validate their outputs within bounded execution environments. As these agents assume broader, more autonomous responsibilities, their operational success relies heavily on the contextual scaffolding provided to them. Repository-level instruction files, most notably the rapidly standardizing AGENTS.md format, serve as the primary control plane for defining agent behavior, establishing architectural constraints, and setting procedural expectations.For researchers and practitioners designing controlled experiments to measure the precise impact of scaffolding instructions on agent performance, understanding the empirical landscape of prompt engineering for software tasks is an absolute necessity. The current literature, spanning 2023 to early 2026, presents a highly complex matrix of established best practices, detrimental anti-patterns, and areas of profound scientific uncertainty. This report exhaustively analyzes the empirical evidence surrounding AI coding agent instructions, identifies the dimensions of genuine uncertainty, constructs a scientifically rigorous framework for a fractional factorial experiment, and delineates the confounding variables that threaten the internal validity of agentic evaluations. The analysis specifically distinguishes between empirical evidence derived from human software engineering, evidence from general LLM reasoning tasks applied by analogy, and direct empirical evidence from autonomous AI coding agents.The Empirical Landscape of Scaffolding DimensionsThe efficacy of an AI coding agent is fundamentally tethered to the quality, structure, and semantic density of its system instructions. Recent empirical studies evaluating agents on rigorous execution-based benchmarks—such as SWE-bench, SWE-bench Mobile, and various proprietary enterprise datasets—have revealed that the content of scaffolding files significantly alters not only the functional correctness of the generated code but also the agent's process efficiency, commonly measured by token consumption, execution time, and tool-call iterations. To determine which aspects of scaffolding instructions possess the strongest empirical evidence of affecting output quality, efficiency, or process, one must systematically dissect the primary dimensions of agent interaction.Context Files and the Minimalism ImperativeThe adoption of repository-level context files, such as AGENTS.md or CLAUDE.md, has accelerated rapidly as a mechanism to provide agents with project-specific knowledge that cannot be inferred from the raw source code. However, direct empirical evidence from AI coding agents yields a highly nuanced, and sometimes seemingly contradictory, view of their effectiveness. This reveals a critical relationship between instruction density and the cognitive efficiency of the underlying LLM.An empirical study analyzing agent executions on real-world GitHub pull requests demonstrated that the presence of an AGENTS.md file can significantly improve operational efficiency. In a paired within-task design, agents provided with these files exhibited a 28.64% decrease in median wall-clock execution time and a 16.58% reduction in output token consumption, all while maintaining comparable task completion rates. This direct evidence from AI coding agents suggests that highly targeted context—such as specific build commands, dependency installation steps, and test execution scripts—accelerates task completion by preventing the agent from relying on trial-and-error exploration to discover the repository's mechanics.Conversely, rigorous evaluations on the AGENTbench dataset, which specifically targets the impact of actively used context files, indicate that auto-generated or overly verbose context files frequently degrade agent performance. Across multiple frontier models, the inclusion of comprehensive context files reduced task success rates compared to providing no repository context at all, while simultaneously driving up inference costs by over 20%. Behavioral trace analysis from these studies provides the mechanistic explanation: dense, sprawling context files trigger excessive and unwarranted exploration. Agents subjected to long AGENTS.md files perform broader file traversals and unnecessary testing cycles in a misguided attempt to satisfy all stated guidelines, even those irrelevant to the specific issue at hand.The synthesis of these findings points to a clear, evidence-backed conclusion regarding context management. Context files are highly beneficial when they provide missing, strictly necessary operational knowledge, but they become actively detrimental when padded with generic architectural philosophies, aspirational coding standards, or redundant project overviews. The literature strongly advocates for "minimal requirements," establishing that unnecessary constraints impose a cognitive load that directly impairs the model's fundamental problem-solving capabilities.Output Formatting and Cognitive EfficiencyThe structural format mandated by scaffolding instructions heavily influences both the accuracy of the agent's reasoning and its computational efficiency. While JSON and XML have long been the industry standards for programmatic data exchange and API integrations in human software engineering, empirical evidence from general LLM reasoning tasks reveals severe limitations when these formats are applied to generative AI outputs.Stress-testing advanced models like GPT-5, Llama 3.2, and Gemini 2.5 Flash Lite across different output formats demonstrated that Markdown is universally the most token-efficient format, consuming 34% to 38% fewer tokens than JSON. Furthermore, YAML and Markdown consistently yield higher parsing success and reasoning accuracy than XML or JSON. This discrepancy occurs because the syntax of Markdown and YAML aligns much more closely with the natural language text corpora that dominate the pre-training data of large language models. XML performed the poorest in these evaluations, requiring 80% more tokens than Markdown for identical data payloads, thereby nearly doubling inference costs and exacerbating context window exhaustion without providing any concomitant gain in structural reliability. Therefore, evidence from LLM reasoning tasks strongly indicates that scaffolding instructions should mandate Markdown for any structured output or intermediate planning steps.Constraint Engineering and Scope ProtectionAI coding agents, optimized through Reinforcement Learning from Human Feedback (RLHF) to be maximally helpful, exhibit a strong behavioral bias toward implementing unstated requirements or refactoring stable code unprompted. This behavior necessitates explicit scope protection within scaffolding instructions. The "DO NOT CHANGE" pattern has emerged as a critical safeguard in production environments. Because AI models struggle to infer boundaries from omission, instructions must positively state non-goals. If a specification does not explicitly forbid the modification of an existing database schema, an agent might spontaneously "improve" it, leading to cascading system failures.Direct empirical evidence from the SWE-bench Mobile benchmark further supports the superiority of focused, restrictive constraints over abstract guidance. In a systematic ablation study comparing various prompt engineering strategies for complex repository tasks, simple "Defensive Programming" prompts outperformed highly complex, multi-layered prompt templates by 7.4%. The successful defensive programming templates explicitly directed the agent to avoid dynamic hacks, omit placeholder code, and focus on strict type boundaries. This demonstrates that concrete, restrictive instructions yield higher task success than verbose architectural guidance, reinforcing the principle that agents perform best when their operational envelope is tightly and explicitly bounded.Role Definitions and Synthetic PersonasThe use of synthetic personas—instructing an LLM to "Act as a Senior Software Engineer" or "Act as an Expert System Architect"—is a pervasive prompt engineering technique borrowed from early conversational AI research. However, empirical studies examining the impact of personas on complex coding tasks present mixed results. While some studies indicate that persona guidance can enhance code generation accuracy in isolated, function-level tasks by aligning the model's output distribution with higher-quality training data clusters , broader evaluations question their ecological validity in realistic software engineering workflows.A comprehensive study analyzing the interactions between various prompt techniques (such as personas, chain-of-thought, and few-shot examples) revealed that while personas influence the stylistic generated code, combining multiple techniques does not necessarily improve functional correctness and can even degrade performance. Furthermore, invoking a "senior" persona often causes the agent to over-engineer solutions for simple tasks, introducing unnecessary abstractions that complicate maintainability. Thus, while personas alter the stylistic flavor of the output, direct evidence from AI coding agents suggests they are less critical to functional correctness than explicit architectural constraints or minimal context definitions.Delineating Certainty and Genuine UncertaintyTo design a highly valuable, scientifically rigorous controlled experiment, one must isolate variables where the literature demonstrates conflicting evidence or a distinct lack of consensus. Testing established truths yields little scientific value, while testing definitively disproven methods introduces unnecessary failure modes into the experimental baseline. The design of the $2^3$ fractional factorial experiment must carefully separate the knowns from the unknowns.The Fixed Baseline: What Demonstrates Clear Evidence of EfficacyThe experimental baseline must incorporate practices that are empirically proven to stabilize agent behavior and improve performance. These elements should be hardcoded into the system prompt and remain entirely constant across all six experimental runs to prevent them from acting as confounding variables.Markdown Output Formatting: The output formatting requirements for any intermediate planning, reasoning, or structured data must utilize Markdown. Evidence from LLM reasoning tasks unequivocally proves that Markdown maximizes parsing accuracy and minimizes token consumption compared to JSON or XML.Explicit Scope Constraints: The scaffolding must include explicit negative constraints. Bounding the agent's scope by explicitly listing non-goals and utilizing "DO NOT CHANGE" directives is proven to prevent the hallucination of unrequested features and protects existing repository architecture from unwarranted agentic refactoring.Instruction Minimalism: The overarching philosophy of the baseline instructions must strictly adhere to the principle of minimalism. The files should provide only the essential environmental context—such as specific npm build commands, testing scripts, and absolute core dependencies. The baseline must strictly avoid extraneous stylistic guidelines or aspirational architectural philosophies, which direct evidence from AI agents shows will cause exploratory thrashing and context rot.The Exclusions: What Demonstrates Clear Evidence of InefficacyConversely, several practices have been empirically shown to degrade performance, increase costs without benefit, or induce failure modes. These must be entirely excluded from the experimental design.Verbose Repository Summaries: The inclusion of LLM-generated, boilerplate context files that summarize the entire project architecture or list hundreds of general coding standards must be excluded. These files consistently reduce success rates by forcing the agent to align its output with overly broad, non-actionable heuristics, thereby diluting its attention on the actual task.Strict XML/JSON Schemas for Reasoning: Forcing the agent to enclose its intermediate chain-of-thought reasoning within complex XML tags or strict JSON schemas should be avoided due to the severe token inefficiency and the documented negative impact on the LLM's logical reasoning accuracy.Complex Multi-Agent Orchestration for Unit Tasks: For a medium-complexity, single-package task, designing an instruction file that attempts to force the single model to simulate a complex multi-agent debate (e.g., simulating a coder, a reviewer, and a tester simultaneously in one context window) introduces massive coordination overhead. Empirical evidence indicates that multi-agent coordination often fails at high rates due to context fragmentation and conflicting internal logic.Areas of Genuine Scientific UncertaintyOnce the baseline is established and the proven anti-patterns are excluded, the experimental design must focus on the fault lines in contemporary AI software engineering research. The literature reveals deep contradictions and genuine scientific uncertainty in three primary domains: execution workflows, validation methodologies, and the nature of architectural constraint. These three domains represent the optimal dimensions for the $2^3$ factorial experiment.The first area of profound uncertainty lies in the orchestration of the agent's reasoning workflow, specifically the dichotomy between interleaved reactive execution and sequential upfront planning. The "ReAct" (Reasoning and Acting) paradigm allows agents to dynamically alternate between thinking and executing tool calls. This approach closely mimics human debugging, providing high adaptability to unexpected errors from the compiler or the environment. However, direct evidence from coding agents shows that ReAct architectures are highly prone to entering infinite tool-calling loops, losing sight of the overarching objective, and consuming massive amounts of tokens. In contrast, "Plan-then-Execute" architectures force the agent to generate a comprehensive, multi-step roadmap before taking any action. Empirical studies from LLM reasoning tasks indicate that Plan-then-Execute can reduce total token consumption by up to 50% by minimizing redundant execution loops and forcing holistic context processing. Yet, strict planning frameworks often exhibit severe brittleness; if an early assumption in the plan proves incorrect during execution, the agent struggles to dynamically replan, leading to elevated failure rates. The comparative efficacy of these two workflows within a single-agent AGENTS.md file remains highly contested and untested in this specific operational context.The second area of genuine uncertainty involves validation methodologies, specifically the tension between Test-Driven Development (TDD) and Implementation-First approaches. In human software engineering, TDD is widely championed as the optimal paradigm, with empirical studies historically demonstrating significant defect reduction and improved architectural modularity. Extending this logic to AI, many theoretical frameworks advocate for strict test-first constraints to prevent agent regression and ground the agent's output in verifiable metrics. However, emerging direct empirical evidence from AI coding agents highlights a severe pathological failure mode when LLMs attempt TDD, termed "Context Pollution" or the tautology problem. Because LLMs are fundamentally next-token predictors trained on open-source repositories where implementation code almost always precedes the corresponding tests, their internal weights are heavily biased toward writing tests that simply confirm whatever logic is currently in context. When explicitly instructed to write tests first, agents frequently write highly coupled, implementation-aware tests that will pass even if the core business logic is fundamentally flawed, creating a false sense of correctness. Consequently, it remains highly uncertain whether forcing a coding agent into a strict RED-GREEN-REFACTOR cycle yields higher objective mutation scores than allowing it to implement the logic naturally and subsequently write tests to cover the generated paths.The third area of uncertainty revolves around the spectrum of architectural guidance, specifically contrasting highly restrictive defensive programming constraints against broader, idiomatic freedom. As noted previously, synthetic personas are common, but their impact on complex logic is debated. Instructing an agent to adopt a "Senior TypeScript Engineer" persona aims to implicitly invoke high-quality, community-standard design patterns, allowing the LLM to leverage the most robust statistical clusters in its training data. However, other studies suggest that giving the model too much freedom leads to inconsistent abstractions and vulnerabilities. Research from the SWE-bench Mobile benchmark indicates that explicitly dictating strict, defensive programming rules—such as forcing explicit null checks and forbidding dynamic typing—significantly outperforms generic quality prompts. It is fundamentally uncertain whether a pure-logic npm package (such as a state machine) benefits more from the rigid, micro-managed safety of defensive constraints or the fluid, statistically probable elegance of idiomatic freedom.Experimental Design: Formulating the Instruction VariantsGiven the constraints of a single agent, a single model, and a single medium-complexity specification (a Node.js/TypeScript state machine), the experiment permits a $2^3$ balanced fractional factorial design. This allows for the precise manipulation of three binary dimensions across six distinct runs. The selected dimensions—Workflow Paradigm, Validation Strategy, and Code Quality Philosophy—are practically relevant for practitioners crafting AGENTS.md files, are highly likely to produce measurable effect sizes in efficiency and correctness, and are orthogonal to one another, thereby minimizing confounding interaction effects.For each dimension of uncertainty, the following tables define the specific pair of instruction variants that constitute a meaningful, concrete comparison. These variants are designed to be directly inserted into the AGENTS.md scaffolding file.Dimension 1: Workflow Paradigm (Plan-then-Execute vs. Iterative ReAct)This dimension tests the fundamental cognitive loop the agent utilizes to solve the state machine specification. It evaluates whether forced upfront deliberation yields higher process quality and token efficiency than dynamic, reactive exploration. This dimension is entirely orthogonal to how the code is tested or styled, affecting primarily the git commit history (process quality), the total token consumption, and the frequency of agent restarts.VariantConcrete Instruction FormulationRationale for ComparisonA: Plan-then-Execute"Before writing any code, creating any files, or executing any shell commands, you must first output a complete, step-by-step implementation plan in Markdown. This plan must detail every file to be created and the exact sequence of operations. You must strictly execute this plan linearly without deviation. Do not interleave planning and execution."Tests the empirical claim from LLM reasoning studies that separating deliberation from execution reduces token consumption and prevents infinite loops, directly measuring if the forced structured reasoning overcomes the agent's inherent brittleness when facing unexpected environmental feedback.B: Iterative ReAct"Approach this implementation iteratively. For every step of the process, you must follow a Thought-Action-Observation cycle. Write a brief thought about what you need to do, execute a single tool or make a single code change, and observe the terminal or compiler result. Continuously adjust your approach based on the immediate feedback until the specification is met."Tests the hypothesis derived from agentic frameworks that continuous, state-grounded feedback produces higher code correctness by allowing the agent to dynamically pivot and recover from compiler errors, measuring if this adaptability justifies the expected increase in token consumption.Dimension 2: Validation Strategy (Test-Driven vs. Implementation-First)This dimension directly addresses the highly debated "Context Pollution" phenomenon observed in LLM test generation. By altering the chronological order of file creation and validation, this dimension directly impacts objective code correctness (mutation testing scores and test pass rates), and the behavioral traces visible in the repository's commit history (file creation order).VariantConcrete Instruction FormulationRationale for ComparisonA: Test-Driven Development (TDD)"You must strictly follow Test-Driven Development (TDD). First, write comprehensive, failing unit tests for the state machine logic based solely on the specification. Execute the tests and verify they fail. Only after verifying failure may you write the implementation code. Refactor your code until all tests pass."Evaluates whether forcing an artificial constraint—writing tests before logic, which contradicts the dominant distribution of the LLM's training data—successfully guides the agent to higher mutation scores, or if it triggers the documented failure mode of tautological test generation.B: Implementation-First"Focus entirely on implementing the state machine logic first. Write clean, functional code that meets all acceptance criteria in the specification. Only after the core implementation is complete should you write a comprehensive suite of unit tests to verify your logic against the specification."Evaluates whether aligning the instruction with the LLM's pre-training data bias (code before tests) reduces cognitive friction, prevents context pollution, and ultimately yields more accurate implementations and higher-quality test suites.Dimension 3: Architectural Guidance (Defensive Constraints vs. Idiomatic Freedom)This dimension evaluates how architectural constraints dictate the agent's output at the syntax level. It compares the efficacy of highly restrictive, defensive constraints against broader, idiomatic guidance. This dimension primarily influences the abstract syntax tree, the handling of edge cases in the business day calculations, and the overall robustness of the pure-logic package.VariantConcrete Instruction FormulationRationale for ComparisonA: Defensive Constraints"Apply strict defensive programming principles. You must validate all inputs at the public boundaries, explicitly check for undefined or null states before proceeding, avoid all dynamic type casting (never use any), and implement exhaustive error handling and bounds checking for every state transition and business day calculation."Tests the empirical findings from SWE-bench Mobile that explicit, restrictive defensive guidelines yield the highest task success rates by removing ambiguity and forcing the agent to handle edge cases that it might statistically ignore.B: Idiomatic Freedom"Act as an Expert TypeScript Engineer. Write elegant, idiomatic, and highly readable code. Rely on standard TypeScript design patterns and functional programming concepts. Optimize for code simplicity, modularity, and maintainability, trusting standard language features over verbose, manual error checking."Tests whether invoking an expert persona with the freedom to choose standard architectural patterns results in better structural cohesion and fewer generated bugs than attempting to micromanage the agent's syntax through rigid rules.Methodological Confounds and Threats to ValidityWhile the proposed $2^3$ fractional factorial design successfully isolates the independent variables of interest, several systemic confounds inherent to LLM-based agents threaten the internal validity of the study. If these confounds are not tightly controlled or explicitly acknowledged in the experimental methodology, they may skew the metrics for correctness, efficiency, and process quality, rendering the comparative analysis invalid.The Instruction Length and Structural Complexity ConfoundThe most significant threat to the experiment's internal validity is the variation in instruction length and structural complexity across the different scaffolding variants. Recent empirical studies utilizing specialized metrics, such as the IFScale benchmark and the SustainScore, demonstrate a pronounced inverse relationship between instruction length and an LLM's ability to reliably follow those instructions. As the absolute number of constraints, guidelines, or keywords increases, models exhibit either a linear or exponential decay in performance.If the TDD instruction variant requires 150 words to adequately explain the necessary RED-GREEN-REFACTOR cycle, while the Implementation-First variant requires only 40 words, any observed drop in the TDD agent's performance may not be due to the methodological efficacy of TDD itself. Rather, it is highly likely a result of the cognitive overload associated with processing a longer prompt. This phenomenon, frequently referred to in the literature as "Context Rot" or "Context Pollution," dictates that the model's attention mechanism degrades as the context window expands, causing it to ignore earlier instructions, hallucinate conflicting guidelines, or suffer from primacy bias (heavily favoring instructions at the very beginning of the prompt).To rigorously mitigate this confound, the token count and structural complexity of the contrasting variants must be meticulously normalized. The paired instructions must possess a nearly identical word count, utilize the same grammatical complexity, and employ a matched number of imperative verbs. By controlling the linguistic density, the researcher ensures that differences in agent performance are strictly attributable to the semantic meaning of the workflow, rather than the mechanical parsing burden of the text.Interaction with Task Difficulty and Domain SpecificityThe findings generated by this experiment are inherently bound to the specific nature of the chosen task. The experimental setup specifies a "medium-complexity pure-logic npm package" involving a state machine with approximately 10 states and business day calculations. Empirical evaluations of AI coding agents demonstrate massive variability in performance depending heavily on the task type and the domain. For instance, large-scale analyses of autonomous pull requests reveal that agents excel at localized algorithmic logic, new feature generation in greenfield projects, and documentation updates, but struggle profoundly with cross-file dependencies, deep architectural refactoring, and legacy code integration.Therefore, an instruction variant that proves superior for a localized, pure-logic state machine—such as the Plan-then-Execute workflow—might fail catastrophically if applied to a legacy codebase requiring subtle, multi-file configuration changes where ReAct's exploratory nature is absolutely necessary. The results regarding efficiency and process quality will be highly valid for isolated, algorithmic greenfield projects, but extrapolating these conclusions to repository-scale maintenance, complex UI development, or tasks requiring heavy I/O operations introduces a severe generalization confound. The resulting thesis must explicitly scope its claims to pure-logic, zero-dependency generation tasks.Model-Specific Architectural and Training BiasesThe internal architecture and post-training regimen of the chosen LLM introduces a latent variable that cannot be entirely neutralized. Different frontier models are aligned using distinct preference optimization techniques—such as Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), or Group Relative Policy Optimization (GRPO)—and undergo highly specialized instruction tuning. Consequently, different models exhibit unique sensitivities to specific prompting structures and task chronologies.For example, models demonstrate varying decay patterns in instruction following; some models maintain high accuracy until a specific token limit is reached (threshold decay), while others decay linearly from the first instruction. Furthermore, the underlying pre-training corpus exerts a powerful gravitational pull on the agent's behavior. If the underlying model was heavily fine-tuned on Python datasets where a specific testing paradigm or object-oriented structure was dominant, applying that same model to a Node.js/TypeScript task might trigger unexpected functional biases. A model trained heavily on agentic, tool-using trajectories might inherently favor a ReAct approach regardless of the instructions provided, subconsciously resisting a rigid Plan-then-Execute directive. Because the experiment utilizes a single, fixed model across all runs, the observed optimal configurations (e.g., discovering that TDD combined with Defensive Programming yields the highest mutation score) may be highly over-fit to that specific model's latent space, rather than representing a universal principle for all AI coding agents.Evaluator Bias and Metrics ValidityFinally, the metrics chosen to evaluate code correctness and process quality must be absolutely resilient against automated evaluation biases. If an "LLM-as-a-judge" framework is utilized to assess the quality of the git commit messages, the elegance of the generated architecture, or the clarity of the code, it introduces a well-documented "length bias." Empirical research shows that LLM evaluators consistently and disproportionately favor verbose, longer outputs regardless of their actual factual or architectural quality, often confusing polite or expansive wording with higher engineering rigor.To counter this, relying strictly on deterministic, execution-based metrics—such as the exact mutation testing score (to measure test suite robustness), the deterministic pass rate of the test suite against a hidden human-written oracle suite, and the absolute count of token consumption—is imperative. However, the researcher must recognize that even deterministic metrics can be misleading if not carefully analyzed. For example, a 100% test pass rate generated under the Implementation-First variant might simply reflect the agent's ability to write shallow, tautological tests that do not exercise the core edge cases of the business day calculations, creating a false equivalence in correctness between the variants. Therefore, the mutation testing score is the most critical metric, as it objectively measures whether the agent's validation strategy actually catches regressions, bypassing the tautology trap entirely.ConclusionConstructing optimal scaffolding files for AI coding agents is an exercise in cognitive engineering. It is less about providing exhaustive, human-readable documentation and more about engineering precise, minimal contexts that align flawlessly with the underlying model's computational architecture and training biases. The current empirical landscape reveals that while certain practices—such as Markdown formatting, minimal context inclusion, and explicit scope protection—are universally beneficial, fundamental questions regarding agent workflows, validation chronologies, and architectural constraints remain deeply contested.By executing a $2^3$ fractional factorial experiment that rigorously contrasts Plan-then-Execute against ReAct workflows, Test-Driven Development against Implementation-First strategies, and Defensive Constraints against Idiomatic Freedom, this research design directly addresses the most pressing uncertainties in Software Engineering 3.0. Crucially, by meticulously controlling for instruction length, recognizing task-specific limitations, and relying on deterministic mutation testing rather than biased LLM evaluators, the resulting data will offer highly valid, actionable insights. These findings will not only advance the academic understanding of AI agent behavior but will provide software practitioners with empirical blueprints for authoring highly effective AGENTS.md files, fundamentally improving the reliability and efficiency of autonomous coding assistants in production environments.