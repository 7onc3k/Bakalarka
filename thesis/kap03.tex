\chapter{Metodika}
\label{kap:metodika}

\section{Výběr projektu pro case study}

\begin{raw}
Práce se zaměřuje na řízení a scaffolding - jde více do šířky než do hloubky. Proto potřebujeme menší projekt, na kterém můžeme spustit více běhů s různými nastaveními scaffoldingu a měřit výsledky.

Pro experiment potřebujeme projekt který:
\begin{itemize}
    \item \textbf{Hard logic} - jasná business pravidla, ne subjektivní výstupy (např. generování textu)
    \item \textbf{Jasné invarianty} - deterministické chování, matematicky ověřitelné správnost
    \item \textbf{Testovatelné} - lze objektivně měřit kvalitu výstupu
    \item \textbf{Přiměřená velikost} - menší projekt umožňuje více experimentálních běhů
    \item \textbf{Reálný use case} - prakticky využitelné, ne umělý příklad
\end{itemize}
\end{raw}

\subsection*{Systém upomínek faktur}

\begin{raw}
Systém pro automatické odesílání připomínek k nezaplaceným fakturám. Obsahuje:
\begin{itemize}
    \item Stavový automat pro sledování stavu faktury (nová, po splatnosti, upomínaná, eskalovaná)
    \item Časové výpočty (pracovní dny, ochranné lhůty)
    \item Pravidla pro eskalaci (kdy poslat další upomínku, kdy předat k vymáhání)
    \item Plánování odesílání upomínek
\end{itemize}
\end{raw}

\section{Zvolená metodika: Spec-Driven Development}

\begin{raw}
Pro referenční implementaci i experimenty je zvolena metodika \textbf{Spec-Driven Development (SDD)}
na úrovni \textbf{spec-first} \cite{sdd2026}.

\textbf{Zdůvodnění volby:}
\begin{enumerate}
    \item \textbf{Specifikace řídí implementaci} -- kvalita specifikace přímo ovlivňuje kvalitu výstupu agenta (viz 2.4.2 bod 7). SDD formalizuje tento princip.
    \item \textbf{Waterfall per increment} -- každý issue = jeden increment s detailní specifikací, ale mezi incrementy iterativní přístup. Odpovídá micro-waterfall hypotéze (2.3.4) podpořené empirickými daty \cite{watanabe2025agentprs, ehsani2026failedprs}.
    \item \textbf{Malé, focused úkoly} -- empirická data ukazují že malé agent PRs mají vyšší úspěšnost \cite{ehsani2026failedprs}. SDD spec-first přirozeně vede k dekompozici na testovatelné incrementy.
    \item \textbf{Spec-first stačí} -- pro jednorázový experiment není potřeba spec-anchored (udržovat sync spec-kód). Spec se napíše, agent implementuje, vyhodnotí se.
\end{enumerate}

\textbf{SDD workflow v kontextu BP:}
\begin{enumerate}
    \item \textbf{Specify} -- napsat GitHub Issue se strukturovanou specifikací (šablona viz níže)
    \item \textbf{Plan} -- (pro agenty: agent sám plánuje; pro referenci: autor plánuje)
    \item \textbf{Implement} -- implementace podle specifikace
    \item \textbf{Validate} -- testy (unit, acceptance criteria), review
\end{enumerate}
\end{raw}

\section{Referenční implementace}

\begin{draft}
Referenční implementace slouží dvěma účelům: validaci specifikace a~vytvoření
\textbf{behavioral test suite}, která se následně spouští na výstupech všech
experimentálních běhů jako objektivní metrika correctness.

\textbf{Metoda: TDD z acceptance criteria.}
Postup odpovídá spec-first TDD \cite{mathews2024}: nejprve se napíší
behavioral testy přímo z~AC ve formátu Given/When/Then -- testy jsou
zpočátku červené (implementace neexistuje). Následně se implementuje
dunning system tak, aby testy postupně zelenaly. Tato sekvence zajišťuje,
že expected values pochází ze specifikace, nikoliv z~pozorování kódu
(test oracle problem \cite{mathews2024}).

\textbf{Behavioral testy, ne unit testy.}
Referenční testy testují chování systému přes veřejné API (vstupy a~výstupy
definované specifikací), nikoliv interní implementaci. Tento přístup
odpovídá black-box testování funkčních požadavků \cite{swebok2024}
a~metodice SWE-bench \cite{swebench2024}, která používá behavioral test
suite jako primární metriku pro hodnocení AI agentů. Výhodou je
přenositelnost: stejné testy lze spustit na implementaci libovolného
agentního běhu bez znalosti jeho interní struktury.

\textbf{Validace specifikace.}
Implementace referenčního řešení odhaluje nejednoznačnosti a~mezery
v~acceptance criteria dříve než experimentální běhy. Pokud referenční
implementace narazí na nedostatečně specifikované chování, AC se
upřesní -- to zaručuje, že všechny experimentální běhy pracují
se stejnou, kompletní specifikací.

\textbf{Výstup:} behavioral test suite (TypeScript/Vitest) spustitelná
na libovolné implementaci se standardizovaným vstupním bodem API
(viz Technical Requirements v~Issue~\#1). Test suite tvoří základ
metriky correctness pro všechny experimentální běhy R0--R5.
\end{draft}

\rule{\textwidth}{0.4pt}

\textbf{Formát specifikace: GitHub Issues}

Specifikace referenční implementace je strukturována jako GitHub Issues. Volba tohoto formátu vychází z:

\begin{enumerate}
    \item \textbf{Akademický standard} -- SWE-bench \cite{swebench2024}, de facto benchmark pro AI coding agenty (ICLR 2024), používá GitHub Issues jako specifikaci. 2294 úloh z reálných repozitářů.
    \item \textbf{Agilní RE praxe} -- v agilních týmech user stories a backlog items nahrazují formální SRS dokumenty \cite{cao2008}.
    \item \textbf{Open source praxe} -- issue trackery fungují jako de facto requirements management \cite{scacchi2002}.
    \item \textbf{Nativní čitelnost pro agenty} -- agent čte issues přes GitHub API nebo CLI, propojuje je s branches a PR.
    \item \textbf{Traceability} -- Issue \#N $\rightarrow$ branch $\rightarrow$ commits $\rightarrow$ PR $\rightarrow$ merge. Přirozená provázanost specifikace s implementací \cite{gotel1994}.
\end{enumerate}

Struktura každého issue vychází z empirického výzkumu o optimální specifikaci pro LLM agenty
(viz sekce 2.4.2, bod 7). Studie ukazují, že kvalita requirements přímo koreluje s kvalitou
LLM výstupu \cite{rope2024} a že tradiční user stories jsou příliš abstraktní pro přímý
vstup do LLM \cite{ullrich2025} -- je nutná dekompozice a obohacení o konkrétní kontext.

\textbf{Dvě vrstvy specifikace:}

Původní návrh obsahoval tři vrstvy (requirements, specification, architecture).
Analýza redundance (viz níže) ukázala, že prostřední vrstva (specification: inputs/outputs,
pre/postconditions) je implicitně obsažena v~acceptance criteria a~invariantech.
Výsledná šablona proto obsahuje dvě vrstvy:

\begin{enumerate}
    \item \textbf{Requirements} (problémová doména -- CO business potřebuje):
    \begin{itemize}
        \item Title, Description -- účel a kontext funkcionality
        \item Acceptance criteria -- Given/When/Then s~konkrétními hodnotami \cite{ticoder2024}.
              Implicitně obsahují vstupy/výstupy (Given/Then), pre/postconditions
              (Given = precondition, Then = postcondition) i~přechody stavů.
              Jsou přímo mapovatelná na unit testy -- explicitní test cases tedy
              nejsou nutnou součástí specifikace, ale odvozitelným artefaktem
        \item Domain glossary -- sdílený slovník z business domény \cite{domaincodegen2024}
    \end{itemize}

    \item \textbf{Architecture} (struktura -- JAK je řešení organizované):
    \begin{itemize}
        \item Type definitions -- datové typy, interfaces, enums \cite{wen2024io, specine2025}
        \item Invariants -- business pravidla která musí vždy platit \cite{newcomb2025prepost}
        \item Behavioral model -- state diagram, sekvenční logika \cite[kap.~5.4]{sommerville2016}
        \item Technické constraints -- tech stack, patterns, rozhraní
    \end{itemize}
\end{enumerate}

Toto rozdělení slouží jako základ pro experimentální dimenzi ``specifikace'' (viz sekce Experimenty):
referenční implementace používá plnou specifikaci (obě vrstvy),
experimenty variují úroveň detailu -- Full (obě vrstvy) vs. Minimal (pouze requirements).

\textbf{Zdůvodnění redukce z~tří na dvě vrstvy:}

Původní třívrstvý návrh obsahoval prostřední vrstvu \textit{Specification}
(inputs/outputs, pre/postconditions). Analýza ukázala překryv s~ostatními vrstvami:
acceptance criteria implicitně obsahují vstupy/výstupy (Given/Then),
pre/postconditions (Given = precondition, Then = postcondition) i~přechody stavů.
Tato redundance představuje problém: pro člověka vyšší cognitive overload,
pro LLM agenta plýtvání vzácným context window duplicitními informacemi.

Anthropic \cite{anthropic2025context} zavádí pojem \textbf{context rot} --
s~rostoucím počtem tokenů klesá schopnost modelu přesně vzpomínat informace.
Doporučuje ``nejmenší možnou sadu high-signal tokenů''. IEEE 830 \cite{ieee830}
upozorňuje, že ``redundance sama o sobě není chyba, ale snadno k~chybám vede''.
Bockeler \cite{bockeler2025sdd} kritizuje spec-kit (GitHub) za to, že specifikační
soubory jsou ``repetitive, both with each other, and with the code'' --
označuje to jako \textit{Verschlimmbesserung} (zhoršení snahou o~zlepšení).

Obsah zrušené vrstvy byl absorbován: vstupy/výstupy do acceptance criteria
(konkrétní hodnoty v~Given/When/Then), datové typy do \textit{type definitions}
a~pre/postconditions do \textit{invariants} v~architektonické vrstvě.

\textbf{Dvě publikum, různé potřeby:}

Specifikace slouží dvěma publikům současně: \textbf{AI agentovi} (implementuje
z~ní kód) a \textbf{lidskému vývojáři} (rozumí co se staví a kontroluje
co agent vytvořil). Kruchtenův 4+1 model \cite{kruchten1995} argumentuje,
že více pohledů je komplementárních \textbf{pro různá publika}.
Diagramy jsou pro člověka ``high-bandwidth'' komunikace (rychlé pochopení
celkové struktury), zatímco LLM zpracovávají Mermaid diagramy jako text.
Konkrétní Given/When/Then scénáře mohou být pro agenta účinnější
než vizuální model, ale pro člověka méně přehledné u~komplexních systémů.

Experimenty mohou ukázat optimální kombinaci elementů --
ne ``čím víc, tím lépe'', ale \textbf{která minimální sada}
reprezentací je efektivní pro obě publika současně.

\textbf{Zasazení do SASE frameworku:}

Hassan et al. \cite{hassan2025sase} navrhují framework Structured Agentic Software
Engineering (SASE), který rozlišuje \textbf{SE4H} (SE for Humans -- člověk jako
``Agent Coach'' zaměřený na intent, strategii a mentoring) a \textbf{SE4A}
(SE for Agents -- strukturované prostředí pro agenty). Definují tři typy
artefaktů: \textbf{BriefingScript} (mission brief -- co agent má udělat),
\textbf{LoopScript} (workflow playbook -- jak má postupovat)
a~\textbf{MentorScript} (quality normy -- jaké standardy dodržovat).

Naše specifikační šablona odpovídá BriefingScript: obsahuje intent (Description),
ověřitelná kritéria (Acceptance Criteria) a~doménový kontext (Glossary).
Soubor \texttt{agents.md} se scaffoldingem odpovídá LoopScript a~MentorScript:
definuje workflow (git conventions, testování) a~kvalitativní normy (code quality).
Experimentální dimenze ``úroveň detailu specifikace'' přímo testuje to, co
Hassan et al. nazývají \textbf{duality of control} -- kdy dát agentovi strukturu
a~kdy ho nechat rozhodovat autonomně.

Kruchtenův Scenarios (+1) view \cite{kruchten1995}, který sloužil jako validační
most mezi všemi pohledy pro všechny stakeholdery, nachází paralelu
v~acceptance criteria -- ty fungují jako most mezi záměrem člověka a~exekucí agenta.

\textbf{Empirické pořadí důležitosti:}

Studie Specine \cite{specine2025} empiricky měřila dopad jednotlivých elementů
na kvalitu generovaného kódu (Pass@1, 4 LLM, 5 benchmarků):

\textit{Tier 1 -- nejvyšší dopad:}
\begin{itemize}
    \item Příklady s vysvětlením ($\sim$14.5\%) $\rightarrow$ Acceptance criteria
    \item Účel specifikace ($\sim$13.5\%) $\rightarrow$ Description
    \item Výstupní požadavky ($\sim$11.6\%) $\rightarrow$ Outputs
\end{itemize}

\textit{Tier 2 -- silně doporučené:}
vstupní požadavky, klíčové pojmy, edge/corner cases.

\textit{Tier 3 -- hodnotné pro složité úlohy:}
pre/postconditions \cite{newcomb2025prepost}, error handling, behavioral model.

Tato šablona kombinuje přístupy podložené výzkumem:
structured natural language \cite[kap.~4.4]{sommerville2016}, test-driven specifikaci \cite{ticoder2024},
doménový kontext \cite{domaincodegen2024}, redukci specification misalignment \cite{specine2025},
design constraints \cite{newcomb2025prepost} a klarifikaci ambiguity \cite{clarifygpt2024}.

\section{Experimenty}

\begin{raw}
\subsection{Experimentální design}

Experiment používá \textbf{vyvážený frakční faktoriální design} porovnávající
dva instrukční styly -- \textit{meta} (co + proč) a~\textit{explicit}
(co + jak) -- napříč třemi dimenzemi scaffoldingu. Meta-instrukce říkají
agentovi co udělat a~proč je to důležité; agent si sám zvolí konkrétní
přístup. Explicit instrukce říkají co udělat a~jak na to jít podle
literatury; agent následuje doporučenou metodu. Šest běhů (R0--R5)
pokrývá všechny kombinace tak, že každá dimenze je testována
3$\times$ jako meta a~3$\times$ jako explicit.
Referenční implementace (human-guided) slouží jako ground truth pro metriky.

\begin{raw}
[RAW]
\textbf{Volba experimentálního přístupu:}

Při návrhu experimentu byly zváženy tři přístupy k~ablaci scaffoldingu:

\textit{(1) Vrstvová ablace} (SWE-agent \cite{yang2024sweagent},
Confucius \cite{wang2025cca}): odebírání celých vrstev (CI, agent configs,
spec detail). Problém: vrstvy míchají fáze SDLC -- odebrání ``procesních
instrukcí'' zasahuje do Requirements (TDD), Design (workflow) i~Implementation
(git konvence) současně. Nelze izolovat přínos konkrétní komponenty.

\textit{(2) Fázová ablace}: organizace podle SDLC fází (Requirements $\to$
Design $\to$ Implementation). Čistější rámec, ale naráží na cascade
dependencies -- Requirements ablace závisí na tom co bylo v~Design fázi.
Izolace vyžaduje fixaci jedné dimenze a~variaci druhé, což exponenciálně
zvyšuje počet běhů.

\textit{(3) Porovnání instrukčních stylů}: agent dostane pro každou
dimenzi scaffoldingu instrukci buď ve stylu \textit{meta} (co + proč)
nebo \textit{explicit} (co + jak). Meta-instrukce říkají co udělat
a~proč (motivace), agent si zvolí metodu sám. Explicit instrukce říkají
co udělat a~jak (metoda z~literatury), agent následuje doporučený přístup.
Výhody: (a)~testuje přímo otázku z~literatury -- Suzgun
\cite{suzgun2024metaprompt} a~Tessl.io tvrdí meta $>$ explicit,
Lulla \cite{lulla2026} ukazuje efektivitu explicitních arch. informací,
(b)~vyvážený faktoriální design umožňuje párové srovnání per dimenze,
(c)~žádná studie dosud neporovnala tyto instrukční styly u~coding agentů.

Zvolen přístup (3). Přístupy (1) a~(2) jsou pokryty existující literaturou
(SWE-agent abluje ACI komponenty, Confucius abluje scaffolding features,
FeatureBench \cite{featurebench2026} abluje spec tiers). Přístup (3)
přináší nový metodologický příspěvek -- přímé porovnání instrukčních
stylů.

\textbf{Dva instrukční styly:}

Experiment porovnává dva přístupy k~instruování coding agenta,
odvozené ze tří nezávislých zdrojů v~literatuře:

\textit{Meta (co + proč):} Instrukce stanoví cíl a~motivaci; agent si
zvolí konkrétní metodu sám. Odpovídá Directive + Context komponentám
\cite{promptstotemplates2025}, goal-level meta-promptingu
\cite{suzgun2024metaprompt} a~BriefingScript v~SASE
\cite{hassan2025sase}.

\textit{Explicit (co + jak):} Instrukce stanoví cíl a~doporučenou
metodu z~literatury; agent následuje předepsaný přístup. Odpovídá
Directive + Method \cite{promptstotemplates2025}, task-specific
promptingu \cite{suzgun2024metaprompt} a~LoopScript v~SASE
\cite{hassan2025sase}.

Tento design je motivován třemi zjištěními: (1)~Suzgun a~Kalai
\cite{suzgun2024metaprompt} ukázali, že meta-prompting přináší
17,1\,\% zlepšení; (2)~Lulla et al. \cite{lulla2026} ukázali, že
explicitní architektonické informace v~AGENTS.md snižují runtime
o~28\,\%; (3)~Breunig \cite{breunig2026} demonstroval, že záměna
system promptů mění chování agenta více než volba modelu. Žádná studie
dosud neporovnala meta a~explicit instrukční styly per dimenze
u~coding agentů.

\textbf{Fixní proměnné (stejné pro všechny běhy):}
\begin{itemize}
    \item Prázdné GitHub repo (pouze \texttt{AGENTS.md},
          \texttt{.opencode/config.json}, auto-continue plugin)
    \item Specifikace v~GitHub Issue \#1 -- plné acceptance criteria
          a~doménový glossary, bez architecture layer
    \item Auto-continue plugin (\texttt{session.idle} hook s~počítadlem
          restartů a~build/test kontrolou)
    \item Model: GLM-5 přes OpenCode (system prompt \texttt{qwen.txt}
          se základními pravidly -- no comments, run lint/typecheck)
    \item Fixní hlavička AGENTS.md: odkaz na specifikaci, popis prostředí
          (OpenCode + dostupné nástroje), výstupní požadavek
\end{itemize}

\textbf{Standardizovaná struktura instrukce:}

Oba instrukční styly mají identickou strukturu -- variuje obsah
a~abstrakční úroveň. Meta-instrukce obsahuje cíl + motivaci (proč),
explicit instrukce obsahuje cíl + metodu (jak). Obě končí constraintem.

\begin{center}
\begin{tabular}{p{2cm}|p{5cm}|p{5cm}}
\textbf{Sekce} & \textbf{Meta (co + proč)} & \textbf{Explicit (co + jak)} \\
\hline
Instrukce & Cíl na procesní úrovni & Cíl + konkrétní metoda z~literatury \\
\hline
Why / How & Motivace a~rationale & Principy a~postupy \\
\hline
Constraint & Co nedělat & Co nedělat \\
\end{tabular}
\end{center}

Standardizace struktury izoluje proměnnou (instrukční styl) od formátu.
Prompt sensitivity \cite{razavi2025} je přiznána jako limitation;
mitigace: identická struktura, srovnatelná délka, literature-backed
obsah obou stylů.

\textbf{Tři dimenze meta-instrukcí:}

Původní návrh obsahoval pět granulárních meta-instrukcí (sub-agents,
architektura, dekompozice, workflow, quality tooling). Analýza nezávislosti
ukázala, že dekompozice, role separation, workflow a~testovací strategie
jsou facety téhož procesu (What/Who/How/When organizace práce) -- nejsou
vzájemně nezávislé a~jejich izolovaná ablace by vytvářela nesmyslné
konfigurace (např. sub-agenti bez issues k~delegaci).

Výsledný design používá tři ortogonální dimenze, odvozené systematicky
ze tří zdrojů: (1)~SASE framework \cite{hassan2025sase} rozlišuje
BriefingScript (fixní specifikace), LoopScript $\to$~O, MentorScript/AEE
$\to$~Q a~architektonické rozhodnutí $\to$~P; (2)~Wang et al.
\cite{wang2024llmagentsse} klasifikují agentní aktivity na Team
Organization $\to$~O, Decision Making $\to$~P, Testing/QA $\to$~Q;
(3)~RepairAgent \cite{bouzenia2025repairagent} abluje funkčně nezávislé
komponenty -- náš design toto respektuje.

\begin{center}
\begin{tabular}{p{0.5cm}|p{2cm}|p{4.2cm}|p{4.2cm}}
\textbf{Dim.} & \textbf{Název} & \textbf{Meta (co + proč)} &
\textbf{Explicit (co + jak)} \\
\hline
P & Planning/ Design &
  Navrhni strukturu řešení před kódem. \textit{Proč:} ad-hoc rozhodnutí
  vedou k~nekonzistentní struktuře (Perry \& Wolf 1992). &
  Navrhni strukturu pomocí: information hiding (Parnas 1972),
  type-driven design, separation of concerns, interface-first. \\
\hline
O & Organization &
  Rozděl práci na sub-tasky, odděl role, TDD ze specifikace.
  \textit{Proč:} feature-level úlohy selhávají (FeatureBench 2026),
  test oracle problem (Mathews 2024). &
  Dekomponuj dle závislostí (FeatureBench 2026), odděl role
  (Wang 2024), TDD ze specifikace (Mathews 2024), trackuj postup. \\
\hline
Q & Quality &
  Nastav quality feedback před implementací.
  \textit{Proč:} bez zpětné vazby se chyby kumulují (Papadakis 2019). &
  Nastav: static analysis + formatter, coverage gates,
  CI pipeline (Lulla 2026), mutation testing (Papadakis 2019). \\
\end{tabular}
\end{center}

Každá dimenze operuje na jiné úrovni: P~rozhoduje \textit{co} stavět
(technická struktura), O~rozhoduje \textit{jak} pracovat (organizace
procesu), Q~rozhoduje \textit{čím} kontrolovat kvalitu (feedback
mechanismy). Tato ortogonalita zajišťuje, že odebrání jedné dimenze
neovlivní logiku ostatních.

\textbf{Experimentální běhy (vyvážený frakční faktoriální design):}

\begin{center}
\begin{tabular}{p{0.7cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{5.5cm}}
\textbf{Run} & \textbf{P} & \textbf{O} & \textbf{Q} &
\textbf{Popis} \\
\hline
R0 & meta & meta & meta &
  Plně meta baseline \\
\hline
R1 & meta & meta & explicit &
  Explicit quality (industry standards) \\
\hline
R2 & meta & explicit & explicit &
  Explicit organizace + quality \\
\hline
R3 & explicit & meta & meta &
  Explicit planning (design principles) \\
\hline
R4 & explicit & explicit & meta &
  Explicit planning + organizace \\
\hline
R5 & explicit & explicit & explicit &
  Plně explicit baseline \\
\end{tabular}
\end{center}

Šest běhů, single run per podmínku. Každá dimenze je testována přesně
3$\times$ jako meta a~3$\times$ jako explicit (dokonalá balance).
Design umožňuje čisté párové srovnání per dimenze -- pro každou dimenzi
existují dva páry běhů lišící se pouze v~dané dimenzi:
P: R0$\leftrightarrow$R3 a~R2$\leftrightarrow$R5;
O: R1$\leftrightarrow$R2 a~R3$\leftrightarrow$R4;
Q: R0$\leftrightarrow$R1 a~R4$\leftrightarrow$R5.
Celkové srovnání instrukčních stylů: R0 (plně meta) vs. R5 (plně explicit).

Každý běh probíhá v~separátním GitHub repozitáři (čistý stav).
Artefakty (commit history, issues, PR) zachovány pro post-hoc analýzu.

\textbf{Metriky:}
\begin{itemize}
    \item \textbf{Primární:} mutation score (Stryker), test pass rate
          (referenční testy), počet restartů (auto-continue)
    \item \textbf{Sekundární:} spotřeba tokenů, čas do dokončení,
          compliance a~alignment (LLM-as-a-judge)
    \item \textbf{Behavioral trace:} co agent reálně udělal -- navrhl
          architekturu před kódem~(P)? rozložil práci na sub-issues~(O)?
          nastavil CI a~linting~(Q)? Ověřuje, že instrukční styl měl
          skutečný efekt na chování agenta a~porovnává přístupy
          zvolené při meta vs. explicit instrukci.
\end{itemize}

\textbf{Pilot studie:} Před experimentálními běhy proběhne pilotní validace
setupu na R0. Pilot identifikuje technické problémy (timeouty, bugy
v~orchestraci, nedostatky v~šabloně) a~kalibruje metriky. Iterace na
pilotu se nepočítají do výsledků studie.

\textbf{Deployment a~Maintenance mimo scope:} experiment měří co agent
vytvoří; npm package nemá reálné uživatele ani provoz.

\textbf{Proč tři SDLC fáze (bez Testing):} Testing fáze se v~kontextu
TDD rozpadá na dvě aktivity: psaní testů = interpretace requirements
(spustitelná specifikace \cite{sommerville2016, swebok2024}),
spouštění testů = verifikační feedback loop implementace. Test oracle
problem \cite{mathews2024} potvrzuje, že testy odvozené z~kódu (nikoliv
ze specifikace) validují chybné chování v~68,1\,\% případů.
\end{raw}

\textbf{Katalog ablatable komponent (brainstorm):}

Následující seznam identifikuje komponenty, které lze jednotlivě odebrat
a~měřit jejich dopad. Vychází z~empirických ablačních studií v~literatuře
-- SWE-agent \cite{yang2024sweagent} abluje ACI komponenty,
CCA \cite{wang2025cca} abluje scaffolding features,
Anthropic \cite{anthropic2025harness} identifikuje failure-specific komponenty.

\textit{Specifikace (BriefingScript):}
\begin{itemize}
    \item Architecture vrstva (types, invariants, behavioral model) \cite{specine2025}
    \item Doménový glossary \cite{domaincodegen2024}
    \item Detail acceptance criteria (konkrétní hodnoty vs. obecný popis) \cite{ticoder2024}
    \item Behavioral model / state diagram \cite{sommerville2016}
\end{itemize}

\textit{Instrukce (LoopScript + MentorScript):}
\begin{itemize}
    \item \texttt{agents.md} jako celek \cite{hassan2025sase}
    \item TDD instrukce (testy před implementací) \cite{mathews2024}
    \item Git workflow konvence (branching, commits, PR)
    \item Code quality standardy (strict mode, no-any)
    \item Code review instrukce
\end{itemize}

\textit{Prostředí (Agent Execution Environment):}
\begin{itemize}
    \item CI pipeline (automatická kontrola)
    \item Pre-konfigurovaný tooling (tsconfig, eslint, vitest, stryker)
    \item Project structure (adresářová struktura, package.json)
\end{itemize}

\textit{Orchestrace a~session management:}
\begin{itemize}
    \item Session-per-issue granularita -- empiricky validovaná jako optimální
          scope \cite{spotify2025context}; agenti selhávají při feature-level
          scope \cite{featurebench2026}
    \item Issue-based task decomposition -- dekompozice úkolů do sub-issues
          s~dependencies; task decomposition jako context management
          strategie \cite{chainofagents2024}
    \item Sub-agent delegace -- orchestrátor deleguje issues na sub-agenty
          s~čistým kontextem; hierarchická orchestrace dosahuje SOTA
          \cite{agentorchestra2025}; trust calibration mezi delegátorem
          a~delegátem \cite{googledelegation2026}
    \item Role separation -- oddělení analytických a~exekučních rolí.
          MASAI \cite{masai2024} definuje 5~specializovaných sub-agentů
          se structured artifact passing (ne konverzací); AgentCoder
          \cite{agentcoder2024} empiricky validuje separaci test designu
          od implementace (96,3\,\% HumanEval); Agyn \cite{agyn2026}
          replikuje inženýrský tým s~dedikovanými rolemi a~dosahuje
          72,2\,\% SWE-bench~500. Studie ukazují, že organizační design
          je stejně důležitý jako schopnosti modelu.
    \item Single vs. multi-agent trade-off -- přínos multi-agent architektury
          klesá s~rostoucí schopností modelů \cite{singleormulti2025};
          hybridní přístup (multi-agent jen tam kde je potřeba) přináší
          +1--12\,\% accuracy při --88\,\% cost. Multi-agent systémy
          mají 14~identifikovaných failure modes ve 3~kategoriích
          (system design, inter-agent misalignment, task verification)
          \cite{mast2025}. Minimální efektivní rozdělení = separace
          testování od implementace.
    \item Meta-prompting -- místo explicitních instrukcí agent dostane
          meta-instrukci navrhnout si vlastní workflow; task-agnostic
          scaffolding pattern \cite{suzgun2024metaprompt}; automatizovaný
          design agentních systémů \cite{hu2024adas}
    \item Completion verification loop -- mechanismus zajišťující, že agent
          dokončí celý projekt, ne jen jeden krok. Tři varianty:
          (1)~Ralph Loop -- vnější cyklus s~čistým kontextem per iteraci,
          stav přes soubory/issues, implementován v~Claude Code i~OpenCode;
          (2)~session.idle hook -- plugin uvnitř agentního frameworku,
          při ukončení agenta ověří podmínku dokončení (např. počet
          otevřených issues přes GitHub API) a~pokud není splněna,
          re-injektuje prompt s~aktuálním stavem; počet restartů slouží
          jako metrika autonomie agenta;
          (3)~instruktážní -- agent dostane instrukci ``neskončí dokud
          všechny issues nejsou uzavřeny'', bez vnější kontroly.
    \item Retry strategie -- single-shot vs. fresh-context retry vs.
          child fix issues; scaffolding a~retry jsou nezávisle hodnotné
          a~multiplikativní \cite{neurosymbolic2025}
    \item Strukturovaná dokumentace v~issues -- záznam pro následující
          agent session (co bylo zkušeno, co selhalo, aktuální stav)
          \cite{anthropic2025harness}. Každá inter-agent zpráva je potenciální
          failure point; chain-style error propagation kaskáduje malé nepřesnosti
          do systémových selhání \cite{agentask2025}. Tři validované mechanismy
          předávání stavu: (1)~progress file + git commity \cite{anthropic2025harness},
          (2)~GitHub issues jako komunikační kanál (issue $\to$ PR $\to$ review),
          (3)~structured artifact passing s~formální output specifikací
          \cite{masai2024}
\end{itemize}

\textit{Context window management:}
\begin{itemize}
    \item Context window degraduje s~délkou, ne jen ``dojde'' --
          coding performance klesá z~29\,\% na 3\,\% při long-context
          úlohách \cite{longcodebench2025}; U-shaped křivka, informace
          uprostřed se ztrácí \cite{liu2024lost}; nerovnoměrná degradace
          napříč 18 modely \cite{contextrot2025}
    \item Observation masking (skrytí detailů tool outputu) je stejně
          efektivní jako LLM summarization za poloviční cenu
          \cite{jetbrains2025complexity}
\end{itemize}

Tento katalog byl při návrhu experimentu seskupen do tří ortogonálních
dimenzí: \textbf{P}~(Planning/Design) pokrývá architektonické rozhodnutí,
\textbf{O}~(Organization) pokrývá orchestraci, dekompozici, role separation,
workflow a~testovací strategii, \textbf{Q}~(Quality) pokrývá tooling
a~CI. Položky z~katalogu, které se týkají specifikace (BriefingScript),
jsou fixní proměnnou -- specifikace je stejná pro všechny běhy.
Položky z~context window management jsou infrastrukturní a~řeší je
agentní framework (OpenCode).

\textbf{Multi-issue gap:}

Stávající benchmarky ukazují výrazný propad výkonu agentů při přechodu
od single-issue k~multi-issue úlohám: SWE-EVO \cite{sweevo2025} reportuje
pouze 21\,\% úspěšnost na evolučních úlohách (průměrně 21~souborů) oproti
65\,\% na single-issue SWE-Bench; FeatureBench \cite{featurebench2026} měří
11\,\% na feature-level úlohách; ACE-Bench \cite{acebench2025} 7,5\,\%
na end-to-end feature development. Tato case study (5~issues, celý dunning
system) cílí přesně do tohoto rozsahu, kde scaffolding může přinést
měřitelný rozdíl.

\textbf{Odvozování testů z acceptance criteria:}

Acceptance criteria ve formátu Given/When/Then (BDD) s konkrétními hodnotami
jsou přímo mapovatelná na unit testy. Schopnost agenta korektně odvodit test suite
z~acceptance criteria je měřitelná dimenze experimentu -- odpovídá zjištění
TiCoder \cite{ticoder2024}, kde formalizace záměru přes testy vedla k~45.97\%
zlepšení Pass@1.

\textbf{Strategie testování a mutation testing:}

Klíčovým rizikem LLM-generovaných testů je tzv. \textbf{test oracle problem}
\cite{mathews2024}. Mathews et al. ukázali, že nástroje pro automatické generování
testů (CoverAgent, CoverUp) systematicky filtrují failing testy a~ponechávají
pouze passing -- výsledkem je, že až 68,1\,\% vygenerovaných test suites
\textbf{validuje chybné chování} místo jeho odhalení. Příčinou je, že expected
values jsou odvozeny z~pozorování kódu, ne ze specifikace.

Chen et al. \cite{rethinking2025} empiricky potvrdili na 500 úlohách SWE-bench,
že agent-generované testy slouží primárně jako \textbf{observační feedback}
(value-revealing prints), ne jako validační nástroj -- 83,2\,\% úloh má stejný
výsledek bez ohledu na to, zda agent testy píše. Relační a~boundary kontroly
(nejcennější pro detekci chyb) tvoří pouze 3--8\,\% assertions.

Obrana proti těmto anti-patterns:
\begin{itemize}
    \item \textbf{TDD ze specifikace} -- expected values vycházejí z~acceptance
          criteria, ne z~pozorování kódu. Agent píše testy \textbf{před}
          implementací (red $\rightarrow$ green $\rightarrow$ refactor).
    \item \textbf{Failing test = opravit kód, ne test} -- zabraňuje selection
          biasu, kde se zahazují testy odhalující chyby.
    \item \textbf{Mutation testing jako metrika kvality testů} -- strukturální
          coverage (branch, statement) je nutná, ale nedostatečná podmínka.
          Papadakis et al. \cite{papadakis2019} v~přehledové studii ukázali, že
          mutation score je silnější prediktor detekce reálných chyb než
          strukturální coverage -- 36\,\% chyb je odhalitelných pouze mutation
          testingem. Harman et al. \cite{meta2025} potvrdili v~produkčním
          nasazení na Meta (10\,795 tříd), že 70\,\% mutantů zůstává
          neodhalených i~při plném coverage.
\end{itemize}

Pro referenční implementaci volíme Stryker (mutation testing framework pro
TypeScript) jako objektivní metriku kvality testů agenta. Mutation score měří,
jak dobře testy detekují simulované chyby -- na rozdíl od coverage, která měří
pouze dosažitelnost kódu.

\textbf{Doplňková dimenze -- formát/reprezentace specifikace:}

Nezávisle na úrovni detailu ovlivňuje kvalitu výstupu i~způsob strukturování
informace. Volba kombinovaného formátu (structured text + tabulky + behavioral
model) vychází z~teorie vizuálních notací \cite{moody2009} a~výzkumu informační
hustoty diagramů \cite{larkin1987}. Systematické porovnání formátů je navrženo
jako future work.
\end{raw}

\section{Analýza}

\begin{raw}
[RAW]
Tradiční přístupy k měření kvality SW (podpora pro volbu dimenzí):

\textbf{Sommerville (Ch. 24, s. 705--728):}
\begin{itemize}
    \item Rozlišuje \textbf{control metrics} (procesní -- sledují proces vývoje) vs. \textbf{predictor metrics} (produktové -- měří vlastnosti kódu/dokumentů)
    \item Vztah proces-produkt u SW není přímočarý jako ve výrobě -- SW je designován, ne vyráběn, vliv individuálních dovedností je velký (s. 706)
    \item Produktové metriky (LOC, cyklomatická složitost) nemají jasný a konzistentní vztah ke kvalitativním atributům (s. 721)
    \item → Naše dimenze Functional Quality = predictor metrics, Compliance = control metrics
\end{itemize}

\textbf{McConnell -- Code Complete (Ch. 28, s. 715, Table 28-2):}
\begin{itemize}
    \item Kategorie měření: Size (LOC, třídy, komentáře) a Overall Quality (počet defektů, defekty/KLOC, mean time between failures)
    \item Praktický pohled -- co se dá reálně měřit v projektu
\end{itemize}

\textbf{SWEBOK v4 (Ch. 12, s. 248--256; Ch. 6, s. 176):}
\begin{itemize}
    \item Software Quality Measurement (s. 253) -- kvantifikace atributů pro rozhodování
    \item Míry údržby (s. 176): complexity, maintainability, testability, supportability, reliability
    \item Odkaz na ISO 25010 jako standard pro kvalitativní charakteristiky (s. 46, 256)
\end{itemize}

\textbf{SWE-bench (Appendix C.7, s. 28):}
\begin{itemize}
    \item Cyklomatická složitost (McCabe) a Halstead measures jako metriky pro hodnocení kódu v benchmarku
    \item Příklad jak existující benchmarky měří kvalitu kódu agentů -- ale jen funkční/strukturální, ne procesní
\end{itemize}

\textbf{Jin et al. 2024 -- LLM Agents SWE Survey (Table VII, s. 21):}
\begin{itemize}
    \item Přehled evaluačních metrik: Accuracy, Pass@k, Task Completion Time, Task Success, Execution Accuracy, Win-Rate
    \item Většina existující literatury měří hlavně funkční kvalitu výstupu
    \item → Naše dimenze Compliance a Alignment jsou méně pokryté v literatuře -- vlastní přínos
\end{itemize}
\end{raw}

\begin{raw}
Hodnocení výstupů agentů probíhá ve čtyřech dimenzích: funkční kvalita, procesní kvalita, efektivita a alignment.

\subsection{Functional Quality (Funkční kvalita)}

Měření funkčních vlastností výstupu dle ISO 25010:

\textbf{Completeness (Úplnost):}
\begin{itemize}
    \item Míra pokrytí požadované funkcionality
    \item Měření: Checklist požadavků ze specifikace → procento implementovaných
\end{itemize}

\textbf{Correctness (Správnost):}
\begin{itemize}
    \item Správnost implementace - funguje to jak má?
    \item Měření: Spuštění referenčních testů na kód agenta (pass rate)
    \item Kvalita testů agenta: Mutation testing (Stryker) - mutation score určuje jak dobře testy detekují chyby
\end{itemize}

\subsection{Compliance (Procesní kvalita)}

Dodržování softwarově-inženýrských praktik:

\textbf{Workflow:}
\begin{itemize}
    \item Dodržení flow: issues → branch → commits → PR
    \item Měření: Automatická kontrola git historie a GitHub artefaktů
\end{itemize}

\textbf{Conventions (Konvence):}
\begin{itemize}
    \item Kvalita commit messages (formát, atomicita, srozumitelnost)
    \item Kvalita issues (popis, acceptance criteria)
    \item Kvalita dokumentace a PR description
    \item Měření: LLM-as-a-judge s definovaným rubrikem \cite{llmjudge2024}
\end{itemize}

\textbf{Transparency (Transparentnost):}
\begin{itemize}
    \item Vysvětluje agent svá rozhodnutí?
    \item Dokumentuje postup a důvody?
    \item Měření: LLM-as-a-judge + manuální review
\end{itemize}

\subsection{Efficiency (Efektivita)}

Náklady na dosažení výsledku:

\begin{itemize}
    \item \textbf{Token usage} - spotřeba tokenů (náklady na API)
    \item \textbf{Iterations} - počet pokusů a oprav potřebných k dokončení
    \item \textbf{Time} - celkový čas do dokončení
    \item \textbf{Human intervention} - míra nutných lidských zásahů a korekcí
\end{itemize}

Měření: Logování z agenta a konverzačních sessions.

\subsection{Metody měření}

Kombinace tří přístupů:
\begin{itemize}
    \item \textbf{Automatické} - testy, mutation testing, git log analýza, token counting
    \item \textbf{LLM-as-a-judge} - hodnocení subjektivních aspektů (kvalita commit messages, dokumentace) pomocí LLM s definovaným rubrikem
    \item \textbf{Manuální review} - kvalitativní zhodnocení celku autorem
\end{itemize}

LLM-as-a-judge přístup využívá strukturované hodnocení kde LLM dostane kritéria a škálu, a konzistentně hodnotí všechny běhy. Validace tohoto přístupu probíhá porovnáním s manuálním hodnocením na vzorku \cite{llmjudge2024}.

\subsection{Alignment (Soulad se záměrem)}

Alignment měří, zda agent pochopil skutečný záměr zadání - ne jen doslovnou instrukci, ale co uživatel skutečně chtěl \cite{llmjudge2024}.

Agent může mít 100\% Correctness a Completeness, ale být misaligned - technicky splnil zadání, ale výsledek neodpovídá záměru.

\textbf{Co se hodnotí:}
\begin{itemize}
    \item \textbf{Over-engineering} - přidal agent funkcionalitu která nebyla požadována?
    \item \textbf{Under-delivering} - vynechal agent implicitní požadavky které byly zřejmé z kontextu?
    \item \textbf{Misinterpretation} - pochopil agent zadání špatně?
    \item \textbf{Scope adherence} - držel se agent vymezeného rozsahu?
\end{itemize}

\textbf{Měření:} Manuální review autorem + LLM-as-a-judge porovnávající zadání vs. skutečný výstup.

\subsection{Vyhodnocení}

Identifikace vzorů -- pro které dimenze (P, O, Q) je efektivnější
meta nebo explicit instrukční styl. Porovnání R0 (plně meta) vs. R5
(plně explicit) jako celkové srovnání přístupů. Analýza trade-offs
(např. vyšší kvalita vs. vyšší spotřeba tokenů).

\begin{raw}
[RAW]
\textbf{Rozšíření: ablace instrukčních dimenzí}

Hlavní experiment porovnává meta vs. explicit instrukční styl per dimenze.
Přirozeným rozšířením je \textit{ablace} -- co se stane když dimenzi
odebereme úplně (ani meta, ani explicit)? Ablační běhy by doplnily
faktoriální design o~baseline bez instrukce, což umožní měřit absolutní
přínos každé dimenze nezávisle na stylu.

Toto rozšíření je mimo scope této BP -- vyžaduje další 3--7 běhů.
Je navrženo jako přímé pokračování.
\end{raw}

\begin{raw}
[RAW] TODO: probrat, zatím nápady

\textbf{Retrospektivní kauzální analýza -- hledání ``prvního špatného obratu''}

Místo čekání na zpožděné finální metriky (mutation score, test pass rate)
lze zpětně rekonstruovat \textit{kdy} agent udělal rozhodnutí, které způsobilo
kaskádu problémů. Git log a session transkript (opencode export) tato data
již obsahují -- jde o post-hoc analýzu, ne o změnu designu experimentu.

\textbf{Co extrahovat z git logu:}
\begin{itemize}
    \item Pořadí prvních souborů v \texttt{src/} -- vznikl \texttt{types.ts} před \texttt{transition.ts}?
          (adopce P instrukce)
    \item Vznikly sub-issues před prvním \texttt{.ts} souborem? (adopce O instrukce)
    \item Vznikl CI workflow před první business logikou? (adopce Q instrukce)
    \item Shluky commitů (commit bursts) -- mnoho commitů v krátkém čase = agent bojoval
    \item Reverty a velké přepisy (\texttt{git log -{}-diff-filter=R})
\end{itemize}

\textbf{Co extrahovat ze session transkripu:}
\begin{itemize}
    \item Opakované edity stejného souboru = agent uvízl (proxy pro obtížnost)
    \item Kde agent poprvé vyjádřil nejistotu nebo přepisoval svůj plán
\end{itemize}

\textbf{Mediátorové proměnné (adoption indicators):}
Tři binární hodnoty (jedna per dimenze) měřitelné automaticky z git logu
a GitHub API. Umožňují rozlišit dva scénáře:
(a) instrukce způsobila adoptování praktiky → praktika způsobila lepší výsledek
    (instrukce fungovala přes mechanismus),
(b) agent dosáhl výsledku bez adoptování praktiky (model to měl v weights).
Bez těchto mediátorů lze vidět korelaci treatment → outcome, ale ne mechanismus.

Tato analýza je plně retrospektivní -- data jsou dostupná po spuštění běhů,
žádná změna experimentálního designu není nutná.
\end{raw}
\end{raw}
