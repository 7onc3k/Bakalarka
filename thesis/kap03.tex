\chapter{Metodika}
\label{kap:metodika}

\section{Výběr projektu pro case study}

\begin{raw}
Práce se zaměřuje na řízení a scaffolding - jde více do šířky než do hloubky. Proto potřebujeme menší projekt, na kterém můžeme spustit více běhů s různými nastaveními scaffoldingu a měřit výsledky.

Pro experiment potřebujeme projekt který:
\begin{itemize}
    \item \textbf{Hard logic} - jasná business pravidla, ne subjektivní výstupy (např. generování textu)
    \item \textbf{Jasné invarianty} - deterministické chování, matematicky ověřitelné správnost
    \item \textbf{Testovatelné} - lze objektivně měřit kvalitu výstupu
    \item \textbf{Přiměřená velikost} - menší projekt umožňuje více experimentálních běhů
    \item \textbf{Reálný use case} - prakticky využitelné, ne umělý příklad
\end{itemize}
\end{raw}

\subsection*{Systém upomínek faktur}

\begin{raw}
Systém pro automatické odesílání připomínek k nezaplaceným fakturám. Obsahuje:
\begin{itemize}
    \item Stavový automat pro sledování stavu faktury (nová, po splatnosti, upomínaná, eskalovaná)
    \item Časové výpočty (pracovní dny, ochranné lhůty)
    \item Pravidla pro eskalaci (kdy poslat další upomínku, kdy předat k vymáhání)
    \item Plánování odesílání upomínek
\end{itemize}
\end{raw}

\section{Zvolená metodika: Spec-Driven Development}

\begin{raw}
Pro referenční implementaci i experimenty je zvolena metodika \textbf{Spec-Driven Development (SDD)}
na úrovni \textbf{spec-first} \cite{sdd2026}.

\textbf{Zdůvodnění volby:}
\begin{enumerate}
    \item \textbf{Specifikace řídí implementaci} -- kvalita specifikace přímo ovlivňuje kvalitu výstupu agenta (viz 2.4.2 bod 7). SDD formalizuje tento princip.
    \item \textbf{Waterfall per increment} -- každý issue = jeden increment s detailní specifikací, ale mezi incrementy iterativní přístup. Odpovídá micro-waterfall hypotéze (2.3.4) podpořené empirickými daty \cite{watanabe2025agentprs, ehsani2026failedprs}.
    \item \textbf{Malé, focused úkoly} -- empirická data ukazují že malé agent PRs mají vyšší úspěšnost \cite{ehsani2026failedprs}. SDD spec-first přirozeně vede k dekompozici na testovatelné incrementy.
    \item \textbf{Spec-first stačí} -- pro jednorázový experiment není potřeba spec-anchored (udržovat sync spec-kód). Spec se napíše, agent implementuje, vyhodnotí se.
\end{enumerate}

\textbf{SDD workflow v kontextu BP:}
\begin{enumerate}
    \item \textbf{Specify} -- napsat GitHub Issue se strukturovanou specifikací (šablona viz níže)
    \item \textbf{Plan} -- (pro agenty: agent sám plánuje; pro referenci: autor plánuje)
    \item \textbf{Implement} -- implementace podle specifikace
    \item \textbf{Validate} -- testy (unit, acceptance criteria), review
\end{enumerate}
\end{raw}

\section{Referenční implementace}

\begin{raw}
Experiment má tři úrovně:

\begin{enumerate}
    \item \textbf{Referenční implementace} (human-guided) -- autor dekomponuje
          specifikaci do sub-issues, kontroluje průběh, agent implementuje
          podle předpřipravených issues. Slouží jako ground truth pro metriky
          (referenční testy, mutation score, kvalita kódu).
    \item \textbf{Baseline experiment} (full-scale autonomní) -- agent dostane
          pouze specifikaci (issue \#1) + plný \texttt{agents.md} + plné prostředí.
          Sám dekomponuje, orchestruje, implementuje. Maximum instrukcí,
          žádný lidský zásah. Měří výkon plně autonomního agenta s~nejlepším
          scaffoldingem.
    \item \textbf{Ablační běhy} -- jako baseline, ale s~odebranými komponentami
          scaffoldingu (viz katalog ablatable komponent). Měří dopad
          jednotlivých komponent.
\end{enumerate}

Porovnání referenční vs. baseline ukazuje vzdálenost autonomního agenta
od human-guided vývoje. Porovnání baseline vs. ablace ukazuje dopad
jednotlivých scaffolding komponent.

\rule{\textwidth}{0.4pt}

\textbf{Formát specifikace: GitHub Issues}

Specifikace referenční implementace je strukturována jako GitHub Issues. Volba tohoto formátu vychází z:

\begin{enumerate}
    \item \textbf{Akademický standard} -- SWE-bench \cite{swebench2024}, de facto benchmark pro AI coding agenty (ICLR 2024), používá GitHub Issues jako specifikaci. 2294 úloh z reálných repozitářů.
    \item \textbf{Agilní RE praxe} -- v agilních týmech user stories a backlog items nahrazují formální SRS dokumenty \cite{cao2008}.
    \item \textbf{Open source praxe} -- issue trackery fungují jako de facto requirements management \cite{scacchi2002}.
    \item \textbf{Nativní čitelnost pro agenty} -- agent čte issues přes GitHub API nebo CLI, propojuje je s branches a PR.
    \item \textbf{Traceability} -- Issue \#N $\rightarrow$ branch $\rightarrow$ commits $\rightarrow$ PR $\rightarrow$ merge. Přirozená provázanost specifikace s implementací \cite{gotel1994}.
\end{enumerate}

Struktura každého issue vychází z empirického výzkumu o optimální specifikaci pro LLM agenty
(viz sekce 2.4.2, bod 7). Studie ukazují, že kvalita requirements přímo koreluje s kvalitou
LLM výstupu \cite{rope2024} a že tradiční user stories jsou příliš abstraktní pro přímý
vstup do LLM \cite{ullrich2025} -- je nutná dekompozice a obohacení o konkrétní kontext.

\textbf{Dvě vrstvy specifikace:}

Původní návrh obsahoval tři vrstvy (requirements, specification, architecture).
Analýza redundance (viz níže) ukázala, že prostřední vrstva (specification: inputs/outputs,
pre/postconditions) je implicitně obsažena v~acceptance criteria a~invariantech.
Výsledná šablona proto obsahuje dvě vrstvy:

\begin{enumerate}
    \item \textbf{Requirements} (problémová doména -- CO business potřebuje):
    \begin{itemize}
        \item Title, Description -- účel a kontext funkcionality
        \item Acceptance criteria -- Given/When/Then s~konkrétními hodnotami \cite{ticoder2024}.
              Implicitně obsahují vstupy/výstupy (Given/Then), pre/postconditions
              (Given = precondition, Then = postcondition) i~přechody stavů.
              Jsou přímo mapovatelná na unit testy -- explicitní test cases tedy
              nejsou nutnou součástí specifikace, ale odvozitelným artefaktem
        \item Domain glossary -- sdílený slovník z business domény \cite{domaincodegen2024}
    \end{itemize}

    \item \textbf{Architecture} (struktura -- JAK je řešení organizované):
    \begin{itemize}
        \item Type definitions -- datové typy, interfaces, enums \cite{wen2024io, specine2025}
        \item Invariants -- business pravidla která musí vždy platit \cite{newcomb2025prepost}
        \item Behavioral model -- state diagram, sekvenční logika \cite[kap.~5.4]{sommerville2016}
        \item Technické constraints -- tech stack, patterns, rozhraní
    \end{itemize}
\end{enumerate}

Toto rozdělení slouží jako základ pro experimentální dimenzi ``specifikace'' (viz sekce Experimenty):
referenční implementace používá plnou specifikaci (obě vrstvy),
experimenty variují úroveň detailu -- Full (obě vrstvy) vs. Minimal (pouze requirements).

\textbf{Zdůvodnění redukce z~tří na dvě vrstvy:}

Původní třívrstvý návrh obsahoval prostřední vrstvu \textit{Specification}
(inputs/outputs, pre/postconditions). Analýza ukázala překryv s~ostatními vrstvami:
acceptance criteria implicitně obsahují vstupy/výstupy (Given/Then),
pre/postconditions (Given = precondition, Then = postcondition) i~přechody stavů.
Tato redundance představuje problém: pro člověka vyšší cognitive overload,
pro LLM agenta plýtvání vzácným context window duplicitními informacemi.

Anthropic \cite{anthropic2025context} zavádí pojem \textbf{context rot} --
s~rostoucím počtem tokenů klesá schopnost modelu přesně vzpomínat informace.
Doporučuje ``nejmenší možnou sadu high-signal tokenů''. IEEE 830 \cite{ieee830}
upozorňuje, že ``redundance sama o sobě není chyba, ale snadno k~chybám vede''.
Bockeler \cite{bockeler2025sdd} kritizuje spec-kit (GitHub) za to, že specifikační
soubory jsou ``repetitive, both with each other, and with the code'' --
označuje to jako \textit{Verschlimmbesserung} (zhoršení snahou o~zlepšení).

Obsah zrušené vrstvy byl absorbován: vstupy/výstupy do acceptance criteria
(konkrétní hodnoty v~Given/When/Then), datové typy do \textit{type definitions}
a~pre/postconditions do \textit{invariants} v~architektonické vrstvě.

\textbf{Dvě publikum, různé potřeby:}

Specifikace slouží dvěma publikům současně: \textbf{AI agentovi} (implementuje
z~ní kód) a \textbf{lidskému vývojáři} (rozumí co se staví a kontroluje
co agent vytvořil). Kruchtenův 4+1 model \cite{kruchten1995} argumentuje,
že více pohledů je komplementárních \textbf{pro různá publika}.
Diagramy jsou pro člověka ``high-bandwidth'' komunikace (rychlé pochopení
celkové struktury), zatímco LLM zpracovávají Mermaid diagramy jako text.
Konkrétní Given/When/Then scénáře mohou být pro agenta účinnější
než vizuální model, ale pro člověka méně přehledné u~komplexních systémů.

Experimenty mohou ukázat optimální kombinaci elementů --
ne ``čím víc, tím lépe'', ale \textbf{která minimální sada}
reprezentací je efektivní pro obě publika současně.

\textbf{Zasazení do SASE frameworku:}

Hassan et al. \cite{hassan2025sase} navrhují framework Structured Agentic Software
Engineering (SASE), který rozlišuje \textbf{SE4H} (SE for Humans -- člověk jako
``Agent Coach'' zaměřený na intent, strategii a mentoring) a \textbf{SE4A}
(SE for Agents -- strukturované prostředí pro agenty). Definují tři typy
artefaktů: \textbf{BriefingScript} (mission brief -- co agent má udělat),
\textbf{LoopScript} (workflow playbook -- jak má postupovat)
a~\textbf{MentorScript} (quality normy -- jaké standardy dodržovat).

Naše specifikační šablona odpovídá BriefingScript: obsahuje intent (Description),
ověřitelná kritéria (Acceptance Criteria) a~doménový kontext (Glossary).
Soubor \texttt{agents.md} se scaffoldingem odpovídá LoopScript a~MentorScript:
definuje workflow (git conventions, testování) a~kvalitativní normy (code quality).
Experimentální dimenze ``úroveň detailu specifikace'' přímo testuje to, co
Hassan et al. nazývají \textbf{duality of control} -- kdy dát agentovi strukturu
a~kdy ho nechat rozhodovat autonomně.

Kruchtenův Scenarios (+1) view \cite{kruchten1995}, který sloužil jako validační
most mezi všemi pohledy pro všechny stakeholdery, nachází paralelu
v~acceptance criteria -- ty fungují jako most mezi záměrem člověka a~exekucí agenta.

\textbf{Empirické pořadí důležitosti:}

Studie Specine \cite{specine2025} empiricky měřila dopad jednotlivých elementů
na kvalitu generovaného kódu (Pass@1, 4 LLM, 5 benchmarků):

\textit{Tier 1 -- nejvyšší dopad:}
\begin{itemize}
    \item Příklady s vysvětlením ($\sim$14.5\%) $\rightarrow$ Acceptance criteria
    \item Účel specifikace ($\sim$13.5\%) $\rightarrow$ Description
    \item Výstupní požadavky ($\sim$11.6\%) $\rightarrow$ Outputs
\end{itemize}

\textit{Tier 2 -- silně doporučené:}
vstupní požadavky, klíčové pojmy, edge/corner cases.

\textit{Tier 3 -- hodnotné pro složité úlohy:}
pre/postconditions \cite{newcomb2025prepost}, error handling, behavioral model.

Tato šablona kombinuje přístupy podložené výzkumem:
structured natural language \cite[kap.~4.4]{sommerville2016}, test-driven specifikaci \cite{ticoder2024},
doménový kontext \cite{domaincodegen2024}, redukci specification misalignment \cite{specine2025},
design constraints \cite{newcomb2025prepost} a klarifikaci ambiguity \cite{clarifygpt2024}.
\end{raw}

\section{Experimenty}

\begin{raw}
\subsection{Experimentální design}

Experiment používá \textbf{meta-instrukční ablaci} -- systematické
odebírání meta-instrukcí (co + proč, nikoliv jak) za účelem měření
jejich individuálního přínosu ke kvalitě výstupu agenta. Místo
explicitního scaffoldingu (předpřipravené soubory, konfigurace)
dostane agent sadu meta-instrukcí a~sám si navrhne konkrétní přístup.
Ablace = odebrání jedné dimenze meta-instrukcí. Baseline (R0) obsahuje
všechny tři dimenze; referenční implementace (human-guided) slouží
jako ground truth pro metriky.

\begin{raw}
[RAW]
\textbf{Volba experimentálního přístupu:}

Při návrhu experimentu byly zváženy tři přístupy k~ablaci scaffoldingu:

\textit{(1) Vrstvová ablace} (SWE-agent \cite{yang2024sweagent},
Confucius \cite{wang2025cca}): odebírání celých vrstev (CI, agent configs,
spec detail). Problém: vrstvy míchají fáze SDLC -- odebrání ``procesních
instrukcí'' zasahuje do Requirements (TDD), Design (workflow) i~Implementation
(git konvence) současně. Nelze izolovat přínos konkrétní komponenty.

\textit{(2) Fázová ablace}: organizace podle SDLC fází (Requirements $\to$
Design $\to$ Implementation). Čistější rámec, ale naráží na cascade
dependencies -- Requirements ablace závisí na tom co bylo v~Design fázi.
Izolace vyžaduje fixaci jedné dimenze a~variaci druhé, což exponenciálně
zvyšuje počet běhů.

\textit{(3) Meta-instrukční ablace}: místo explicitního scaffoldingu
(předpřipravené soubory, konfigurace) dostane agent meta-instrukce které
říkají \textit{co} udělat, ale nikoliv \textit{jak}. Ablace = odebrání
jedné meta-instrukce. Výhody: (a)~binární manipulace (instrukce přítomna
nebo ne), (b)~výsledky jsou univerzální -- meta-instrukce nejsou vázané
na konkrétní projekt, (c)~žádná studie dosud neablovala jednotlivé
meta-instrukce u~coding agentů.

Zvolen přístup (3) -- meta-instrukční ablace. Přístupy (1) a~(2) jsou
pokryty existující literaturou (SWE-agent abluje ACI komponenty,
Confucius abluje scaffolding features, FeatureBench \cite{featurebench2026}
abluje spec tiers). Přístup (3) přináší nový metodologický příspěvek.

\textbf{Ablace meta-instrukcí:}

Experiment testuje přínos jednotlivých meta-instrukcí pro kvalitu výstupu
AI agenta. Místo explicitního scaffoldingu (předpřipravené konfigurace,
workflow instrukce, architektura) dostane agent sadu meta-instrukcí --
každá říká \textit{co} udělat a~\textit{proč}, ale nikoliv \textit{jak}.
Agent si na základě meta-instrukce navrhne vlastní přístup.

Tento design je motivován dvěma zjištěními: (1)~Suzgun a~Kalai
\cite{suzgun2024metaprompt} ukázali, že meta-prompting (task-agnostic
scaffolding) přináší 17,1\,\% zlepšení oproti standardnímu promptingu;
(2)~Breunig \cite{breunig2026} empiricky demonstroval, že záměna system
promptů mezi coding agenty mění chování agenta více než volba modelu --
tentýž model s~promptem Codex produkuje metodický workflow, s~promptem
Claude iterativní. Žádná studie dosud neablovala \textit{jednotlivé}
meta-instrukce u~coding agentů -- toto je hlavní metodologický příspěvek
této práce.

\textbf{Fixní proměnné (stejné pro všechny běhy):}
\begin{itemize}
    \item Prázdné GitHub repo (pouze \texttt{package.json},
          \texttt{tsconfig.json})
    \item Specifikace v~GitHub Issue \#1 -- plné acceptance criteria
          a~doménový glossary, bez architecture layer
    \item Auto-continue plugin (\texttt{session.idle} hook s~počítadlem
          restartů)
    \item Model: GLM-5, výchozí temperature OpenCode
    \item Fixní části system promptu: role definition, tool instructions,
          output format, fighting-the-weights instrukce
          \cite{breunig2026}
\end{itemize}

\textbf{Standardizovaná šablona meta-instrukce:}

Každá meta-instrukce má identickou strukturu -- variuje pouze obsah.
Šablona vychází ze systematické analýzy prompt komponent napříč čtyřmi
frameworky \cite{promptstotemplates2025}: Directive (87\,\% prevalence),
Context (56\,\%), Output Format (40\,\%), Constraints (36\,\%).

\begin{verbatim}
[CÍL]: <co udělat>
[KONTEXT]: <proč je to důležité pro kvalitu výstupu>
[VÝSTUP]: <očekávaný artefakt>
[OMEZENÍ]: <co nedělat>
\end{verbatim}

Standardizace eliminuje formulační bias -- všechny instrukce mají
stejnou délku, specifičnost a~strukturu. Prompt sensitivity
\cite{razavi2025} je přiznána jako limitation; mitigace: fixní šablona,
deterministická temperature, behavioral trace.

\textbf{Tři dimenze meta-instrukcí:}

Původní návrh obsahoval pět granulárních meta-instrukcí (sub-agents,
architektura, dekompozice, workflow, quality tooling). Analýza nezávislosti
ukázala, že dekompozice, role separation, workflow a~testovací strategie
jsou facety téhož procesu (What/Who/How/When organizace práce) -- nejsou
vzájemně nezávislé a~jejich izolovaná ablace by vytvářela nesmyslné
konfigurace (např. sub-agenti bez issues k~delegaci).

Výsledný design používá tři ortogonální dimenze, odvozené systematicky
ze tří zdrojů: (1)~SASE framework \cite{hassan2025sase} rozlišuje
BriefingScript (fixní specifikace), LoopScript $\to$~O, MentorScript/AEE
$\to$~Q a~architektonické rozhodnutí $\to$~P; (2)~Wang et al.
\cite{wang2024llmagentsse} klasifikují agentní aktivity na Team
Organization $\to$~O, Decision Making $\to$~P, Testing/QA $\to$~Q;
(3)~RepairAgent \cite{bouzenia2025repairagent} abluje funkčně nezávislé
komponenty -- náš design toto respektuje.

\begin{center}
\begin{tabular}{p{0.5cm}|p{2.5cm}|p{4.5cm}|p{4.5cm}}
\textbf{Dim.} & \textbf{Název} & \textbf{Meta-instrukce (CÍL)} &
\textbf{Očekávané artefakty} \\
\hline
P & Planning/Design &
  Navrhni architekturu řešení před implementací -- typy, moduly, rozhraní &
  Design decisions v~issues, type definitions \\
\hline
O & Organization &
  Zorganizuj si práci -- rozděl na části, definuj role, nastav testovací
  strategii (testy ze specifikace před implementací) &
  Sub-issues, agent configs, workflow v~AGENTS.md \\
\hline
Q & Quality &
  Nastav si quality feedback mechanismy -- linting, testy, CI &
  Nakonfigurované nástroje, CI pipeline \\
\end{tabular}
\end{center}

Každá dimenze operuje na jiné úrovni: P~rozhoduje \textit{co} stavět
(technická struktura), O~rozhoduje \textit{jak} pracovat (organizace
procesu), Q~rozhoduje \textit{čím} kontrolovat kvalitu (feedback
mechanismy). Tato ortogonalita zajišťuje, že odebrání jedné dimenze
neovlivní logiku ostatních.

\textbf{Ablační běhy:}

\begin{center}
\begin{tabular}{p{0.7cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{5.5cm}}
\textbf{Run} & \textbf{P} & \textbf{O} & \textbf{Q} &
\textbf{Co měříme} \\
\hline
R0 & \checkmark & \checkmark & \checkmark &
  Baseline -- plný meta-prompt \\
\hline
R1 & -- & \checkmark & \checkmark &
  Přínos upfront designu
  \cite{perry1992, sommerville2016} \\
\hline
R2 & \checkmark & -- & \checkmark &
  Přínos organizace práce
  \cite{featurebench2026, mathews2024, agentcoder2024} \\
\hline
R3 & \checkmark & \checkmark & -- &
  Přínos quality infrastruktury
  \cite{papadakis2019, meta2025, lulla2026} \\
\end{tabular}
\end{center}

Čtyři běhy, single run per podmínku. Každý běh probíhá v~separátním
GitHub repozitáři (čistý stav). Artefakty (commit history, issues, PR)
zachovány pro post-hoc analýzu. Variace obsahu meta-instrukcí
(meta vs. explicit formulace) je navržena jako future work --
explicit instrukce jsou vysoce citlivé na konkrétní obsah
\cite{razavi2025}, zatímco meta-instrukce jsou robustnější
vůči formulaci.

\textbf{Metriky:}
\begin{itemize}
    \item \textbf{Primární:} mutation score (Stryker), test pass rate
          (referenční testy), počet restartů (auto-continue)
    \item \textbf{Sekundární:} spotřeba tokenů, čas do dokončení,
          compliance a~alignment (LLM-as-a-judge)
    \item \textbf{Behavioral trace:} co agent reálně udělal -- navrhl
          architekturu před kódem~(P)? rozložil práci na sub-issues~(O)?
          nastavil CI a~linting~(Q)? Ověřuje, že ablace meta-instrukce
          měla skutečný efekt na chování agenta.
\end{itemize}

\textbf{Pilot studie:} Před experimentálními běhy proběhne pilotní validace
setupu na R0. Pilot identifikuje technické problémy (timeouty, bugy
v~orchestraci, nedostatky v~šabloně) a~kalibruje metriky. Iterace na
pilotu se nepočítají do výsledků studie.

\textbf{Deployment a~Maintenance mimo scope:} experiment měří co agent
vytvoří; npm package nemá reálné uživatele ani provoz.

\textbf{Proč tři SDLC fáze (bez Testing):} Testing fáze se v~kontextu
TDD rozpadá na dvě aktivity: psaní testů = interpretace requirements
(spustitelná specifikace \cite{sommerville2016, swebok2024}),
spouštění testů = verifikační feedback loop implementace. Test oracle
problem \cite{mathews2024} potvrzuje, že testy odvozené z~kódu (nikoliv
ze specifikace) validují chybné chování v~68,1\,\% případů.
\end{raw}

\textbf{Katalog ablatable komponent (brainstorm):}

Následující seznam identifikuje komponenty, které lze jednotlivě odebrat
a~měřit jejich dopad. Vychází z~empirických ablačních studií v~literatuře
-- SWE-agent \cite{yang2024sweagent} abluje ACI komponenty,
CCA \cite{wang2025cca} abluje scaffolding features,
Anthropic \cite{anthropic2025harness} identifikuje failure-specific komponenty.

\textit{Specifikace (BriefingScript):}
\begin{itemize}
    \item Architecture vrstva (types, invariants, behavioral model) \cite{specine2025}
    \item Doménový glossary \cite{domaincodegen2024}
    \item Detail acceptance criteria (konkrétní hodnoty vs. obecný popis) \cite{ticoder2024}
    \item Behavioral model / state diagram \cite{sommerville2016}
\end{itemize}

\textit{Instrukce (LoopScript + MentorScript):}
\begin{itemize}
    \item \texttt{agents.md} jako celek \cite{hassan2025sase}
    \item TDD instrukce (testy před implementací) \cite{mathews2024}
    \item Git workflow konvence (branching, commits, PR)
    \item Code quality standardy (strict mode, no-any)
    \item Code review instrukce
\end{itemize}

\textit{Prostředí (Agent Execution Environment):}
\begin{itemize}
    \item CI pipeline (automatická kontrola)
    \item Pre-konfigurovaný tooling (tsconfig, eslint, vitest, stryker)
    \item Project structure (adresářová struktura, package.json)
\end{itemize}

\textit{Orchestrace a~session management:}
\begin{itemize}
    \item Session-per-issue granularita -- empiricky validovaná jako optimální
          scope \cite{spotify2025context}; agenti selhávají při feature-level
          scope \cite{featurebench2026}
    \item Issue-based task decomposition -- dekompozice úkolů do sub-issues
          s~dependencies; task decomposition jako context management
          strategie \cite{chainofagents2024}
    \item Sub-agent delegace -- orchestrátor deleguje issues na sub-agenty
          s~čistým kontextem; hierarchická orchestrace dosahuje SOTA
          \cite{agentorchestra2025}; trust calibration mezi delegátorem
          a~delegátem \cite{googledelegation2026}
    \item Role separation -- oddělení analytických a~exekučních rolí.
          MASAI \cite{masai2024} definuje 5~specializovaných sub-agentů
          se structured artifact passing (ne konverzací); AgentCoder
          \cite{agentcoder2024} empiricky validuje separaci test designu
          od implementace (96,3\,\% HumanEval); Agyn \cite{agyn2026}
          replikuje inženýrský tým s~dedikovanými rolemi a~dosahuje
          72,2\,\% SWE-bench~500. Studie ukazují, že organizační design
          je stejně důležitý jako schopnosti modelu.
    \item Single vs. multi-agent trade-off -- přínos multi-agent architektury
          klesá s~rostoucí schopností modelů \cite{singleormulti2025};
          hybridní přístup (multi-agent jen tam kde je potřeba) přináší
          +1--12\,\% accuracy při --88\,\% cost. Multi-agent systémy
          mají 14~identifikovaných failure modes ve 3~kategoriích
          (system design, inter-agent misalignment, task verification)
          \cite{mast2025}. Minimální efektivní rozdělení = separace
          testování od implementace.
    \item Meta-prompting -- místo explicitních instrukcí agent dostane
          meta-instrukci navrhnout si vlastní workflow; task-agnostic
          scaffolding pattern \cite{suzgun2024metaprompt}; automatizovaný
          design agentních systémů \cite{hu2024adas}
    \item Completion verification loop -- mechanismus zajišťující, že agent
          dokončí celý projekt, ne jen jeden krok. Tři varianty:
          (1)~Ralph Loop -- vnější cyklus s~čistým kontextem per iteraci,
          stav přes soubory/issues, implementován v~Claude Code i~OpenCode;
          (2)~session.idle hook -- plugin uvnitř agentního frameworku,
          při ukončení agenta ověří podmínku dokončení (např. počet
          otevřených issues přes GitHub API) a~pokud není splněna,
          re-injektuje prompt s~aktuálním stavem; počet restartů slouží
          jako metrika autonomie agenta;
          (3)~instruktážní -- agent dostane instrukci ``neskončí dokud
          všechny issues nejsou uzavřeny'', bez vnější kontroly.
    \item Retry strategie -- single-shot vs. fresh-context retry vs.
          child fix issues; scaffolding a~retry jsou nezávisle hodnotné
          a~multiplikativní \cite{neurosymbolic2025}
    \item Strukturovaná dokumentace v~issues -- záznam pro následující
          agent session (co bylo zkušeno, co selhalo, aktuální stav)
          \cite{anthropic2025harness}. Každá inter-agent zpráva je potenciální
          failure point; chain-style error propagation kaskáduje malé nepřesnosti
          do systémových selhání \cite{agentask2025}. Tři validované mechanismy
          předávání stavu: (1)~progress file + git commity \cite{anthropic2025harness},
          (2)~GitHub issues jako komunikační kanál (issue $\to$ PR $\to$ review),
          (3)~structured artifact passing s~formální output specifikací
          \cite{masai2024}
\end{itemize}

\textit{Context window management:}
\begin{itemize}
    \item Context window degraduje s~délkou, ne jen ``dojde'' --
          coding performance klesá z~29\,\% na 3\,\% při long-context
          úlohách \cite{longcodebench2025}; U-shaped křivka, informace
          uprostřed se ztrácí \cite{liu2024lost}; nerovnoměrná degradace
          napříč 18 modely \cite{contextrot2025}
    \item Observation masking (skrytí detailů tool outputu) je stejně
          efektivní jako LLM summarization za poloviční cenu
          \cite{jetbrains2025complexity}
\end{itemize}

Tento katalog byl při návrhu experimentu seskupen do tří ortogonálních
dimenzí: \textbf{P}~(Planning/Design) pokrývá architektonické rozhodnutí,
\textbf{O}~(Organization) pokrývá orchestraci, dekompozici, role separation,
workflow a~testovací strategii, \textbf{Q}~(Quality) pokrývá tooling
a~CI. Položky z~katalogu, které se týkají specifikace (BriefingScript),
jsou fixní proměnnou -- specifikace je stejná pro všechny běhy.
Položky z~context window management jsou infrastrukturní a~řeší je
agentní framework (OpenCode).

\textbf{Multi-issue gap:}

Stávající benchmarky ukazují výrazný propad výkonu agentů při přechodu
od single-issue k~multi-issue úlohám: SWE-EVO \cite{sweevo2025} reportuje
pouze 21\,\% úspěšnost na evolučních úlohách (průměrně 21~souborů) oproti
65\,\% na single-issue SWE-Bench; FeatureBench \cite{featurebench2026} měří
11\,\% na feature-level úlohách; ACE-Bench \cite{acebench2025} 7,5\,\%
na end-to-end feature development. Tato case study (5~issues, celý dunning
system) cílí přesně do tohoto rozsahu, kde scaffolding může přinést
měřitelný rozdíl.

\textbf{Odvozování testů z acceptance criteria:}

Acceptance criteria ve formátu Given/When/Then (BDD) s konkrétními hodnotami
jsou přímo mapovatelná na unit testy. Schopnost agenta korektně odvodit test suite
z~acceptance criteria je měřitelná dimenze experimentu -- odpovídá zjištění
TiCoder \cite{ticoder2024}, kde formalizace záměru přes testy vedla k~45.97\%
zlepšení Pass@1.

\textbf{Strategie testování a mutation testing:}

Klíčovým rizikem LLM-generovaných testů je tzv. \textbf{test oracle problem}
\cite{mathews2024}. Mathews et al. ukázali, že nástroje pro automatické generování
testů (CoverAgent, CoverUp) systematicky filtrují failing testy a~ponechávají
pouze passing -- výsledkem je, že až 68,1\,\% vygenerovaných test suites
\textbf{validuje chybné chování} místo jeho odhalení. Příčinou je, že expected
values jsou odvozeny z~pozorování kódu, ne ze specifikace.

Chen et al. \cite{rethinking2025} empiricky potvrdili na 500 úlohách SWE-bench,
že agent-generované testy slouží primárně jako \textbf{observační feedback}
(value-revealing prints), ne jako validační nástroj -- 83,2\,\% úloh má stejný
výsledek bez ohledu na to, zda agent testy píše. Relační a~boundary kontroly
(nejcennější pro detekci chyb) tvoří pouze 3--8\,\% assertions.

Obrana proti těmto anti-patterns:
\begin{itemize}
    \item \textbf{TDD ze specifikace} -- expected values vycházejí z~acceptance
          criteria, ne z~pozorování kódu. Agent píše testy \textbf{před}
          implementací (red $\rightarrow$ green $\rightarrow$ refactor).
    \item \textbf{Failing test = opravit kód, ne test} -- zabraňuje selection
          biasu, kde se zahazují testy odhalující chyby.
    \item \textbf{Mutation testing jako metrika kvality testů} -- strukturální
          coverage (branch, statement) je nutná, ale nedostatečná podmínka.
          Papadakis et al. \cite{papadakis2019} v~přehledové studii ukázali, že
          mutation score je silnější prediktor detekce reálných chyb než
          strukturální coverage -- 36\,\% chyb je odhalitelných pouze mutation
          testingem. Harman et al. \cite{meta2025} potvrdili v~produkčním
          nasazení na Meta (10\,795 tříd), že 70\,\% mutantů zůstává
          neodhalených i~při plném coverage.
\end{itemize}

Pro referenční implementaci volíme Stryker (mutation testing framework pro
TypeScript) jako objektivní metriku kvality testů agenta. Mutation score měří,
jak dobře testy detekují simulované chyby -- na rozdíl od coverage, která měří
pouze dosažitelnost kódu.

\textbf{Doplňková dimenze -- formát/reprezentace specifikace:}

Nezávisle na úrovni detailu ovlivňuje kvalitu výstupu i~způsob strukturování
informace. Volba kombinovaného formátu (structured text + tabulky + behavioral
model) vychází z~teorie vizuálních notací \cite{moody2009} a~výzkumu informační
hustoty diagramů \cite{larkin1987}. Systematické porovnání formátů je navrženo
jako future work.
\end{raw}

\section{Analýza}

\begin{raw}
[RAW]
Tradiční přístupy k měření kvality SW (podpora pro volbu dimenzí):

\textbf{Sommerville (Ch. 24, s. 705--728):}
\begin{itemize}
    \item Rozlišuje \textbf{control metrics} (procesní -- sledují proces vývoje) vs. \textbf{predictor metrics} (produktové -- měří vlastnosti kódu/dokumentů)
    \item Vztah proces-produkt u SW není přímočarý jako ve výrobě -- SW je designován, ne vyráběn, vliv individuálních dovedností je velký (s. 706)
    \item Produktové metriky (LOC, cyklomatická složitost) nemají jasný a konzistentní vztah ke kvalitativním atributům (s. 721)
    \item → Naše dimenze Functional Quality = predictor metrics, Compliance = control metrics
\end{itemize}

\textbf{McConnell -- Code Complete (Ch. 28, s. 715, Table 28-2):}
\begin{itemize}
    \item Kategorie měření: Size (LOC, třídy, komentáře) a Overall Quality (počet defektů, defekty/KLOC, mean time between failures)
    \item Praktický pohled -- co se dá reálně měřit v projektu
\end{itemize}

\textbf{SWEBOK v4 (Ch. 12, s. 248--256; Ch. 6, s. 176):}
\begin{itemize}
    \item Software Quality Measurement (s. 253) -- kvantifikace atributů pro rozhodování
    \item Míry údržby (s. 176): complexity, maintainability, testability, supportability, reliability
    \item Odkaz na ISO 25010 jako standard pro kvalitativní charakteristiky (s. 46, 256)
\end{itemize}

\textbf{SWE-bench (Appendix C.7, s. 28):}
\begin{itemize}
    \item Cyklomatická složitost (McCabe) a Halstead measures jako metriky pro hodnocení kódu v benchmarku
    \item Příklad jak existující benchmarky měří kvalitu kódu agentů -- ale jen funkční/strukturální, ne procesní
\end{itemize}

\textbf{Jin et al. 2024 -- LLM Agents SWE Survey (Table VII, s. 21):}
\begin{itemize}
    \item Přehled evaluačních metrik: Accuracy, Pass@k, Task Completion Time, Task Success, Execution Accuracy, Win-Rate
    \item Většina existující literatury měří hlavně funkční kvalitu výstupu
    \item → Naše dimenze Compliance a Alignment jsou méně pokryté v literatuře -- vlastní přínos
\end{itemize}
\end{raw}

\begin{raw}
Hodnocení výstupů agentů probíhá ve čtyřech dimenzích: funkční kvalita, procesní kvalita, efektivita a alignment.

\subsection{Functional Quality (Funkční kvalita)}

Měření funkčních vlastností výstupu dle ISO 25010:

\textbf{Completeness (Úplnost):}
\begin{itemize}
    \item Míra pokrytí požadované funkcionality
    \item Měření: Checklist požadavků ze specifikace → procento implementovaných
\end{itemize}

\textbf{Correctness (Správnost):}
\begin{itemize}
    \item Správnost implementace - funguje to jak má?
    \item Měření: Spuštění referenčních testů na kód agenta (pass rate)
    \item Kvalita testů agenta: Mutation testing (Stryker) - mutation score určuje jak dobře testy detekují chyby
\end{itemize}

\subsection{Compliance (Procesní kvalita)}

Dodržování softwarově-inženýrských praktik:

\textbf{Workflow:}
\begin{itemize}
    \item Dodržení flow: issues → branch → commits → PR
    \item Měření: Automatická kontrola git historie a GitHub artefaktů
\end{itemize}

\textbf{Conventions (Konvence):}
\begin{itemize}
    \item Kvalita commit messages (formát, atomicita, srozumitelnost)
    \item Kvalita issues (popis, acceptance criteria)
    \item Kvalita dokumentace a PR description
    \item Měření: LLM-as-a-judge s definovaným rubrikem \cite{llmjudge2024}
\end{itemize}

\textbf{Transparency (Transparentnost):}
\begin{itemize}
    \item Vysvětluje agent svá rozhodnutí?
    \item Dokumentuje postup a důvody?
    \item Měření: LLM-as-a-judge + manuální review
\end{itemize}

\subsection{Efficiency (Efektivita)}

Náklady na dosažení výsledku:

\begin{itemize}
    \item \textbf{Token usage} - spotřeba tokenů (náklady na API)
    \item \textbf{Iterations} - počet pokusů a oprav potřebných k dokončení
    \item \textbf{Time} - celkový čas do dokončení
    \item \textbf{Human intervention} - míra nutných lidských zásahů a korekcí
\end{itemize}

Měření: Logování z agenta a konverzačních sessions.

\subsection{Metody měření}

Kombinace tří přístupů:
\begin{itemize}
    \item \textbf{Automatické} - testy, mutation testing, git log analýza, token counting
    \item \textbf{LLM-as-a-judge} - hodnocení subjektivních aspektů (kvalita commit messages, dokumentace) pomocí LLM s definovaným rubrikem
    \item \textbf{Manuální review} - kvalitativní zhodnocení celku autorem
\end{itemize}

LLM-as-a-judge přístup využívá strukturované hodnocení kde LLM dostane kritéria a škálu, a konzistentně hodnotí všechny běhy. Validace tohoto přístupu probíhá porovnáním s manuálním hodnocením na vzorku \cite{llmjudge2024}.

\subsection{Alignment (Soulad se záměrem)}

Alignment měří, zda agent pochopil skutečný záměr zadání - ne jen doslovnou instrukci, ale co uživatel skutečně chtěl \cite{llmjudge2024}.

Agent může mít 100\% Correctness a Completeness, ale být misaligned - technicky splnil zadání, ale výsledek neodpovídá záměru.

\textbf{Co se hodnotí:}
\begin{itemize}
    \item \textbf{Over-engineering} - přidal agent funkcionalitu která nebyla požadována?
    \item \textbf{Under-delivering} - vynechal agent implicitní požadavky které byly zřejmé z kontextu?
    \item \textbf{Misinterpretation} - pochopil agent zadání špatně?
    \item \textbf{Scope adherence} - držel se agent vymezeného rozsahu?
\end{itemize}

\textbf{Měření:} Manuální review autorem + LLM-as-a-judge porovnávající zadání vs. skutečný výstup.

\subsection{Vyhodnocení}

Identifikace vzorů -- které dimenze meta-instrukcí (P, O, Q) měly
největší dopad na kvalitu výstupu agenta. Porovnání trade-offs
(např. vyšší kvalita vs. vyšší spotřeba tokenů).

\begin{raw}
[RAW]
\textbf{Rozšíření: meta vs. explicit instrukce}

Ablační série měří přínos jednotlivých dimenzí meta-instrukcí.
Přirozenou navazující otázkou je: záleží na \textit{formulaci}
instrukce? Meta-instrukce říkají co a~proč (``navrhni architekturu
před implementací''); explicit instrukce říkají přesně jak
(``vytvoř type definitions v~\texttt{src/types.ts}, rozděl moduly
na \texttt{transition.ts} a~\texttt{business-days.ts}'').

Porovnání meta vs. explicit by přímo testovalo SASE \textit{duality
of control} \cite{hassan2025sase} -- kdy dát agentovi strukturu
a~kdy ho nechat rozhodovat autonomně. Explicit instrukce jsou však
vysoce citlivé na konkrétní obsah \cite{razavi2025} -- tentýž záměr
lze formulovat mnoha způsoby s~odlišnými výsledky. Meta-instrukce
jsou robustnější vůči formulaci, protože abstrahují nad konkrétním
provedením.

Toto rozšíření je mimo scope této BP -- vyžaduje vlastní sérii běhů
s~kontrolou formulačního biasu. Je navrženo jako přímé pokračování.
\end{raw}
\end{raw}
