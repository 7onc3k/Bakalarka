\chapter{Metodika}
\label{kap:metodika}

\section{Výběr projektu pro case study}

\begin{raw}
Práce se zaměřuje na řízení a scaffolding - jde více do šířky než do hloubky. Proto potřebujeme menší projekt, na kterém můžeme spustit více běhů s různými nastaveními scaffoldingu a měřit výsledky.

Pro experiment potřebujeme projekt který:
\begin{itemize}
    \item \textbf{Hard logic} - jasná business pravidla, ne subjektivní výstupy (např. generování textu)
    \item \textbf{Jasné invarianty} - deterministické chování, matematicky ověřitelné správnost
    \item \textbf{Testovatelné} - lze objektivně měřit kvalitu výstupu
    \item \textbf{Přiměřená velikost} - menší projekt umožňuje více experimentálních běhů
    \item \textbf{Reálný use case} - prakticky využitelné, ne umělý příklad
\end{itemize}
\end{raw}

\subsection*{Systém upomínek faktur}

\begin{raw}
Systém pro automatické odesílání připomínek k nezaplaceným fakturám. Obsahuje:
\begin{itemize}
    \item Stavový automat pro sledování stavu faktury (nová, po splatnosti, upomínaná, eskalovaná)
    \item Časové výpočty (pracovní dny, ochranné lhůty)
    \item Pravidla pro eskalaci (kdy poslat další upomínku, kdy předat k vymáhání)
    \item Plánování odesílání upomínek
\end{itemize}
\end{raw}

\section{Zvolená metodika: Spec-Driven Development}

\begin{raw}
Pro referenční implementaci i experimenty je zvolena metodika \textbf{Spec-Driven Development (SDD)}
na úrovni \textbf{spec-first} \cite{sdd2026}.

\textbf{Zdůvodnění volby:}
\begin{enumerate}
    \item \textbf{Specifikace řídí implementaci} -- kvalita specifikace přímo ovlivňuje kvalitu výstupu agenta (viz 2.4.2 bod 7). SDD formalizuje tento princip.
    \item \textbf{Waterfall per increment} -- každý issue = jeden increment s detailní specifikací, ale mezi incrementy iterativní přístup. Odpovídá micro-waterfall hypotéze (2.3.4) podpořené empirickými daty \cite{watanabe2025agentprs, ehsani2026failedprs}.
    \item \textbf{Malé, focused úkoly} -- empirická data ukazují že malé agent PRs mají vyšší úspěšnost \cite{ehsani2026failedprs}. SDD spec-first přirozeně vede k dekompozici na testovatelné incrementy.
    \item \textbf{Spec-first stačí} -- pro jednorázový experiment není potřeba spec-anchored (udržovat sync spec-kód). Spec se napíše, agent implementuje, vyhodnotí se.
\end{enumerate}

\textbf{SDD workflow v kontextu BP:}
\begin{enumerate}
    \item \textbf{Specify} -- napsat GitHub Issue se strukturovanou specifikací (šablona viz níže)
    \item \textbf{Plan} -- (pro agenty: agent sám plánuje; pro referenci: autor plánuje)
    \item \textbf{Implement} -- implementace podle specifikace
    \item \textbf{Validate} -- testy (unit, acceptance criteria), review
\end{enumerate}
\end{raw}

\section{Referenční implementace}

\begin{raw}
Experiment má tři úrovně:

\begin{enumerate}
    \item \textbf{Referenční implementace} (human-guided) -- autor dekomponuje
          specifikaci do sub-issues, kontroluje průběh, agent implementuje
          podle předpřipravených issues. Slouží jako ground truth pro metriky
          (referenční testy, mutation score, kvalita kódu).
    \item \textbf{Baseline experiment} (full-scale autonomní) -- agent dostane
          pouze specifikaci (issue \#1) + plný \texttt{agents.md} + plné prostředí.
          Sám dekomponuje, orchestruje, implementuje. Maximum instrukcí,
          žádný lidský zásah. Měří výkon plně autonomního agenta s~nejlepším
          scaffoldingem.
    \item \textbf{Ablační běhy} -- jako baseline, ale s~odebranými komponentami
          scaffoldingu (viz katalog ablatable komponent). Měří dopad
          jednotlivých komponent.
\end{enumerate}

Porovnání referenční vs. baseline ukazuje vzdálenost autonomního agenta
od human-guided vývoje. Porovnání baseline vs. ablace ukazuje dopad
jednotlivých scaffolding komponent.

\rule{\textwidth}{0.4pt}

\textbf{Formát specifikace: GitHub Issues}

Specifikace referenční implementace je strukturována jako GitHub Issues. Volba tohoto formátu vychází z:

\begin{enumerate}
    \item \textbf{Akademický standard} -- SWE-bench \cite{swebench2024}, de facto benchmark pro AI coding agenty (ICLR 2024), používá GitHub Issues jako specifikaci. 2294 úloh z reálných repozitářů.
    \item \textbf{Agilní RE praxe} -- v agilních týmech user stories a backlog items nahrazují formální SRS dokumenty \cite{cao2008}.
    \item \textbf{Open source praxe} -- issue trackery fungují jako de facto requirements management \cite{scacchi2002}.
    \item \textbf{Nativní čitelnost pro agenty} -- agent čte issues přes GitHub API nebo CLI, propojuje je s branches a PR.
    \item \textbf{Traceability} -- Issue \#N $\rightarrow$ branch $\rightarrow$ commits $\rightarrow$ PR $\rightarrow$ merge. Přirozená provázanost specifikace s implementací \cite{gotel1994}.
\end{enumerate}

Struktura každého issue vychází z empirického výzkumu o optimální specifikaci pro LLM agenty
(viz sekce 2.4.2, bod 7). Studie ukazují, že kvalita requirements přímo koreluje s kvalitou
LLM výstupu \cite{rope2024} a že tradiční user stories jsou příliš abstraktní pro přímý
vstup do LLM \cite{ullrich2025} -- je nutná dekompozice a obohacení o konkrétní kontext.

\textbf{Dvě vrstvy specifikace:}

Původní návrh obsahoval tři vrstvy (requirements, specification, architecture).
Analýza redundance (viz níže) ukázala, že prostřední vrstva (specification: inputs/outputs,
pre/postconditions) je implicitně obsažena v~acceptance criteria a~invariantech.
Výsledná šablona proto obsahuje dvě vrstvy:

\begin{enumerate}
    \item \textbf{Requirements} (problémová doména -- CO business potřebuje):
    \begin{itemize}
        \item Title, Description -- účel a kontext funkcionality
        \item Acceptance criteria -- Given/When/Then s~konkrétními hodnotami \cite{ticoder2024}.
              Implicitně obsahují vstupy/výstupy (Given/Then), pre/postconditions
              (Given = precondition, Then = postcondition) i~přechody stavů.
              Jsou přímo mapovatelná na unit testy -- explicitní test cases tedy
              nejsou nutnou součástí specifikace, ale odvozitelným artefaktem
        \item Domain glossary -- sdílený slovník z business domény \cite{domaincodegen2024}
    \end{itemize}

    \item \textbf{Architecture} (struktura -- JAK je řešení organizované):
    \begin{itemize}
        \item Type definitions -- datové typy, interfaces, enums \cite{wen2024io, specine2025}
        \item Invariants -- business pravidla která musí vždy platit \cite{newcomb2025prepost}
        \item Behavioral model -- state diagram, sekvenční logika \cite[kap.~5.4]{sommerville2016}
        \item Technické constraints -- tech stack, patterns, rozhraní
    \end{itemize}
\end{enumerate}

Toto rozdělení slouží jako základ pro experimentální dimenzi ``specifikace'' (viz sekce Experimenty):
referenční implementace používá plnou specifikaci (obě vrstvy),
experimenty variují úroveň detailu -- Full (obě vrstvy) vs. Minimal (pouze requirements).

\textbf{Zdůvodnění redukce z~tří na dvě vrstvy:}

Původní třívrstvý návrh obsahoval prostřední vrstvu \textit{Specification}
(inputs/outputs, pre/postconditions). Analýza ukázala překryv s~ostatními vrstvami:
acceptance criteria implicitně obsahují vstupy/výstupy (Given/Then),
pre/postconditions (Given = precondition, Then = postcondition) i~přechody stavů.
Tato redundance představuje problém: pro člověka vyšší cognitive overload,
pro LLM agenta plýtvání vzácným context window duplicitními informacemi.

Anthropic \cite{anthropic2025context} zavádí pojem \textbf{context rot} --
s~rostoucím počtem tokenů klesá schopnost modelu přesně vzpomínat informace.
Doporučuje ``nejmenší možnou sadu high-signal tokenů''. IEEE 830 \cite{ieee830}
upozorňuje, že ``redundance sama o sobě není chyba, ale snadno k~chybám vede''.
Bockeler \cite{bockeler2025sdd} kritizuje spec-kit (GitHub) za to, že specifikační
soubory jsou ``repetitive, both with each other, and with the code'' --
označuje to jako \textit{Verschlimmbesserung} (zhoršení snahou o~zlepšení).

Obsah zrušené vrstvy byl absorbován: vstupy/výstupy do acceptance criteria
(konkrétní hodnoty v~Given/When/Then), datové typy do \textit{type definitions}
a~pre/postconditions do \textit{invariants} v~architektonické vrstvě.

\textbf{Dvě publikum, různé potřeby:}

Specifikace slouží dvěma publikům současně: \textbf{AI agentovi} (implementuje
z~ní kód) a \textbf{lidskému vývojáři} (rozumí co se staví a kontroluje
co agent vytvořil). Kruchtenův 4+1 model \cite{kruchten1995} argumentuje,
že více pohledů je komplementárních \textbf{pro různá publika}.
Diagramy jsou pro člověka ``high-bandwidth'' komunikace (rychlé pochopení
celkové struktury), zatímco LLM zpracovávají Mermaid diagramy jako text.
Konkrétní Given/When/Then scénáře mohou být pro agenta účinnější
než vizuální model, ale pro člověka méně přehledné u~komplexních systémů.

Experimenty mohou ukázat optimální kombinaci elementů --
ne ``čím víc, tím lépe'', ale \textbf{která minimální sada}
reprezentací je efektivní pro obě publika současně.

\textbf{Zasazení do SASE frameworku:}

Hassan et al. \cite{hassan2025sase} navrhují framework Structured Agentic Software
Engineering (SASE), který rozlišuje \textbf{SE4H} (SE for Humans -- člověk jako
``Agent Coach'' zaměřený na intent, strategii a mentoring) a \textbf{SE4A}
(SE for Agents -- strukturované prostředí pro agenty). Definují tři typy
artefaktů: \textbf{BriefingScript} (mission brief -- co agent má udělat),
\textbf{LoopScript} (workflow playbook -- jak má postupovat)
a~\textbf{MentorScript} (quality normy -- jaké standardy dodržovat).

Naše specifikační šablona odpovídá BriefingScript: obsahuje intent (Description),
ověřitelná kritéria (Acceptance Criteria) a~doménový kontext (Glossary).
Soubor \texttt{agents.md} se scaffoldingem odpovídá LoopScript a~MentorScript:
definuje workflow (git conventions, testování) a~kvalitativní normy (code quality).
Experimentální dimenze ``úroveň detailu specifikace'' přímo testuje to, co
Hassan et al. nazývají \textbf{duality of control} -- kdy dát agentovi strukturu
a~kdy ho nechat rozhodovat autonomně.

Kruchtenův Scenarios (+1) view \cite{kruchten1995}, který sloužil jako validační
most mezi všemi pohledy pro všechny stakeholdery, nachází paralelu
v~acceptance criteria -- ty fungují jako most mezi záměrem člověka a~exekucí agenta.

\textbf{Empirické pořadí důležitosti:}

Studie Specine \cite{specine2025} empiricky měřila dopad jednotlivých elementů
na kvalitu generovaného kódu (Pass@1, 4 LLM, 5 benchmarků):

\textit{Tier 1 -- nejvyšší dopad:}
\begin{itemize}
    \item Příklady s vysvětlením ($\sim$14.5\%) $\rightarrow$ Acceptance criteria
    \item Účel specifikace ($\sim$13.5\%) $\rightarrow$ Description
    \item Výstupní požadavky ($\sim$11.6\%) $\rightarrow$ Outputs
\end{itemize}

\textit{Tier 2 -- silně doporučené:}
vstupní požadavky, klíčové pojmy, edge/corner cases.

\textit{Tier 3 -- hodnotné pro složité úlohy:}
pre/postconditions \cite{newcomb2025prepost}, error handling, behavioral model.

Tato šablona kombinuje přístupy podložené výzkumem:
structured natural language \cite[kap.~4.4]{sommerville2016}, test-driven specifikaci \cite{ticoder2024},
doménový kontext \cite{domaincodegen2024}, redukci specification misalignment \cite{specine2025},
design constraints \cite{newcomb2025prepost} a klarifikaci ambiguity \cite{clarifygpt2024}.
\end{raw}

\section{Experimenty}

\begin{raw}
\subsection{Experimentální design}

Experiment používá \textbf{hypothesis-driven ablation study} -- systematické
odebírání komponent specifikace a~scaffoldingu za účelem měření jejich
individuálního přínosu ke kvalitě výstupu agenta. Na rozdíl od slepého
faktoriálního designu, kde se testují všechny kombinace, každý experimentální
běh ověřuje konkrétní hypotézu odvozenou z~literatury. Jako baseline slouží
referenční implementace (human-guided) s~plnou specifikací a~scaffoldingem.

\textbf{Fixní proměnné:}
\begin{itemize}
    \item \textbf{Model} -- fixní verze LLM pro všechny běhy
    \item \textbf{Task} -- systém upomínek faktur (dunning system)
    \item \textbf{Human intervention} -- žádná (agent pracuje plně autonomně)
    \item \textbf{Orchestrace} -- agent-internal orchestration. Agent dostane celý
          projekt jedním promptem a~sám rozhoduje o~dekompozici, pořadí,
          delegaci na sub-agenty. Vnější mechanismus (\texttt{session.idle} hook)
          pouze ověřuje podmínku dokončení -- pokud zbývají otevřené issues,
          re-injektuje prompt; počet restartů je sbíranou metrikou.
          Orchestrační \textbf{schopnost} agenta je součástí měřeného výstupu,
          orchestrační \textbf{instrukce} v~\texttt{agents.md} jsou součástí
          ablované proměnné (scaffolding).
    \item \textbf{Agent architektura} -- v~referenčním běhu dvou-agentní design:
          (1)~orchestrátor (řízení flow + psaní testů z~AC) a~(2)~executor
          (implementace + verifikace). Separace testování od implementace je
          architektonická garance proti test oracle problému \cite{mathews2024}.
          Agent architektura je zároveň jednou z~ablovaných vrstev (R1) --
          odebrání separace agentů měří přínos cílené role separation.
\end{itemize}

\textbf{Spektrum scaffoldingu:}

Ablace se pohybuje na spektru mezi dvěma extrémy:
\begin{itemize}
    \item \textbf{Maximum} -- detailní \texttt{agents.md} s~procesy pro každou fázi,
          předdefinované sub-issues s~dependencies, přesné instrukce pro TDD,
          git workflow, dokumentaci, code review. Agent exekuje podle předpisu.
    \item \textbf{Minimum} -- pouze specifikace v~GitHub Issue a~prázdný repozitář.
          Agent sám rozhoduje jak dekomponovat, testovat, commitovat, dokumentovat.
\end{itemize}

Tento přístup odpovídá SASE \textit{duality of control} \cite{hassan2025sase}
-- měříme kdy dát agentovi strukturu a~kdy ho nechat rozhodovat autonomně.

\textbf{Dimenze 1: Specifikace (BriefingScript)}

Měří dopad úrovně detailu specifikace na kvalitu výstupu agenta.
V~terminologii SASE \cite{hassan2025sase} odpovídá BriefingScript --
co agent má udělat.

\begin{itemize}
    \item \textbf{Full} -- obě vrstvy (requirements + architecture):
          description, acceptance criteria, glossary, type definitions,
          invariants, behavioral model, technické constraints
    \item \textbf{Minimal} -- pouze vrstva requirements: description
          a~základní acceptance criteria bez architektonických detailů
\end{itemize}

Empiricky podpořeno studiemi \cite{specine2025, rope2024, yang2025underspec}.

\textbf{Dimenze 2: Scaffolding (LoopScript + MentorScript + AEE)}

Měří dopad procesního a~prostředího scaffoldingu na kvalitu výstupu agenta.
V~terminologii SASE odpovídá kombinaci LoopScript (workflow playbook),
MentorScript (quality normy) a~AEE (Agent Execution Environment).

\begin{itemize}
    \item \textbf{Full} -- \texttt{agents.md} s~konvencemi (git workflow,
          commit conventions, testování, code quality) + nakonfigurované
          prostředí (CI pipeline, tsconfig, ESLint, Vitest, project structure)
    \item \textbf{None} -- prázdné repo, pouze specifikace v~GitHub Issue.
          Agent si musí veškeré konvence a~tooling definovat sám.
\end{itemize}

\textbf{Scaffolding jako bundle:}
Procesní instrukce (\texttt{agents.md}) a~nakonfigurované prostředí (CI, linting)
jsou komplementární mechanismy -- \texttt{agents.md} říká ``používej conventional
commits'' a~CI pipeline to vynucuje. V~hlavním experimentu se testují jako celek;
ablace jednotlivých komponent (např. odebrání TDD instrukcí, odebrání CI) je
možným rozšířením.

\textbf{Vrstvová ablace:}

Scaffolding má přirozenou vrstvovou strukturu, kde každá vrstva představuje
jiný typ podpory. Experiment používá izolované ablace po vrstvách -- v~každém
běhu se odebere jedna vrstva, ostatní zůstanou beze změny. To umožňuje měřit
individuální přínos každé vrstvy. Kumulativní běh (R5) ukazuje celkový gap.

\begin{center}
\begin{tabular}{p{0.8cm}|p{4.5cm}|p{3.5cm}|p{3.5cm}}
\textbf{Run} & \textbf{Co agent dostane} & \textbf{Co chybí} & \textbf{Co měříme} \\
\hline
R0 & Vše (spec + agents + CI + tooling) &
  Nic (reference) &
  Baseline pro porovnání \\
\hline
R1 & Jeden agent místo orchestrátor + builder &
  Agentní separace (\texttt{.opencode/agents/}) &
  Přínos role separation \cite{agentcoder2024, singleormulti2025} \\
\hline
R2 & Žádné TDD, phases, when-stuck instrukce &
  Procesní instrukce (LoopScript) &
  Přínos workflow scaffoldingu \cite{hassan2025sase} \\
\hline
R3 & Žádné CI, ESLint, Stryker, coverage thresholds &
  Quality enforcement (AEE) &
  Přínos automatizovaného vynucování \\
\hline
R4 & Vysokoúrovňový popis místo detailních AC &
  Detail specifikace (BriefingScript) &
  Přínos spec detailu \cite{specine2025} \\
\hline
R5 & Pouze \texttt{AGENTS.md} (project context) + spec &
  Vše ostatní (kumulativní) &
  Celkový gap; spontánní self-scaffolding \\
\end{tabular}
\end{center}

R0 (plný scaffolding, autonomní běh bez lidského zásahu) slouží jako referenční
bod pro ablace. R1--R4 jsou \textbf{izolované ablace} -- odebírají jednu vrstvu,
čímž izolují její přínos. R5 je \textbf{kumulativní ablace} -- odebírá vše
kromě project contextu a~specifikace; pokud agent spontánně vytvoří vlastní
workflow (meta-prompting chování), jedná se o~pozorování, ne o~experimentální
manipulaci.

Každý běh probíhá v~separátním GitHub repozitáři vytvořeném ze společného
snapshot (skript \texttt{new-run.sh}). Tím je zaručen čistý stav bez artefaktů
z~předchozích běhů (issues, komentáře, child issues). Artefakty každého běhu
(commit history, PRs, issue komentáře) zůstávají zachovány pro post-hoc analýzu.

\textbf{Pilot studie:} Před experimentálními běhy proběhne pilotní validace
setupu na referenčním běhu (R0). Pilot identifikuje technické problémy
(timeouty, bugy v~orchestraci, nedostatky v~instrukcích) a~kalibruje metriky.
Iterace na pilotu se nepočítají do výsledků studie.

\begin{raw}
[RAW]
\textbf{Návrh: Fázově orientovaný experimentální design (revize R0-R5)}

Vrstvový design R0-R5 odebírá scaffolding globálně (celá vrstva najednou),
ale nezohledňuje, ve které fázi vývoje absence konkrétní komponenty skutečně
škodí. Scaffolding pro Requirements fázi (architecture layer v~spec) je jiný
problém než scaffolding pro Implementation fázi (CI, linting) nebo Design fázi
(issue decomposition).

Revidovaný design je organizován podle tří SDLC fází které agent
v~rámci case study prochází: Requirements $\to$ Design $\to$ Implementation.

\textbf{Proč tři fáze, ne čtyři (Testing):}

Klasická Testing fáze se v~kontextu TDD rozpadá na dvě oddělené aktivity
s~různou povahou. Psaní testů je interpretací požadavků -- testy odvozené
z~acceptance criteria jsou \textit{spustitelná specifikace}, nikoliv
verifikační artefakt \cite{sommerville2016, swebok2024}. Sommerville
explicitně říká, že v~TDD jsou ``testy vyvíjeny spolu s~požadavky
\textit{před} začátkem vývoje'' \cite{sommerville2016}. Spouštění testů
a~CI pipeline naopak slouží jako verifikační feedback loop implementace --
jsou součástí Implementation fáze, nikoliv samostatnou fází.

Tento pohled potvrzuje i~test oracle problem \cite{swebok2024}: test je
validní pouze tehdy, má-li za základ jasnou specifikaci expected outcomes
z~requirements, ne pozorování existujícího kódu. Mathews et al.\
\cite{mathews2024} empiricky ukázali, že 68,1\,\% testů generovaných
z~kódu (nikoliv ze specifikace) validuje chybné chování místo jeho
odhalení -- právě proto, že postrádají kotvení v~requirements.

V~naší architektuře toto odpovídá separaci orchestrátor (test design
ze specifikace = Requirements aktivita) a~builder (test execution =
Implementation verifikace).

\textbf{Deployment a Maintenance mimo scope:} experiment měří co agent
\textit{vytvoří}, nikoliv co se s~artefaktem děje po dokončení;
npm package nemá reálné uživatele ani provoz.

\textbf{Prioritizace podle research gap:}

Literární průzkum identifikoval kde existují empirická data a~kde ne.
Fáze s~menším pokrytím v~literatuře jsou prioritní -- ablace tam přinese
nový příspěvek, nikoliv replikaci známého výsledku.

\textbf{(1) Design fáze -- nejvyšší priorita (research gap)}

Klíčová otázka: dostane agent hotové sub-issues (\#3--\#7, human-prepared)
nebo pouze spec (\#1) a~dekompozici si provede sám?

Empirické porovnání human-prepared vs.\ agent-decomposed neexistuje.
Nepřímý důkaz: MASAI \cite{masai2024} s~5 specializovanými sub-agenty
dosáhl 28,3\,\% na SWE-bench Lite; Confucius \cite{wang2025cca} s~unified
orchestratorem (bez explicitní dekompozice) dosáhl 54,3\,\% na SWE-Bench-Pro.
Tento výsledek \textit{zpochybňuje} intuici, že granulární dekompozice pomáhá
-- může přidávat overhead bez odpovídajícího zisku. Naše case study může
poskytnout první izolovaný test.

\textbf{(2) Requirements fáze -- druhá priorita (kombinovaný efekt znám,
izolace ne)}

Klíčová otázka: přispívá architecture layer (types, invariants, behavioral
model) samostatně, nebo je efekt celé specifikace pohánělý jiným prvkem?

SWE-Bench-Pro \cite{swebenchpro2025} abloval kombinaci requirements +
interface specification a~naměřil pokles z~25,9\,\% na 8,4\,\% (3$\times$
multiplikátor). Ale efekt architecture layer \textit{izolovaně} nebyl měřen.
Invarianty a~stavový diagram jsou specifické pro domain-heavy systémy
(dunning); jejich přínos u~stavového automatu je otevřená otázka.

\textbf{(3) Implementation fáze -- třetí priorita}

Klíčová otázka: zlepší agent výsledek s~pre-konfigurovaným toolingem
(tsconfig strict, ESLint, Stryker) oproti self-setup?

Empirické studie neexistují. AGENTS.md \cite{lulla2026agentsmd} snižuje
runtime o~29\,\%, ale jde o~procesní instrukce, ne tooling konfiguraci.
Hypotéza: pre-konfigurovaný Stryker může motivovat agenta k~lepším testům,
protože mutation score je viditelná metrika v~CI feedbacku.

\textbf{(4) Testing fáze -- nejnižší priorita (dobře pokryto v~literatuře)}

TDD instrukce a~test oracle problem jsou empiricky zdokumentovány
\cite{mathews2024, rethinking2025}. Ablace TDD v~tomto scope by
replikovala existující výsledky bez nového příspěvku.

\textbf{Vztah k~R0-R5:}

Fázový design nenahrazuje R0-R5 kompletně -- spíše ho zpřesňuje.
R0 (reference) zůstává jako baseline. Nové ablační běhy odpovídají
konkrétním fázím: Design ablace = agent dostane jen \#1 (bez \#3--\#7);
Requirements ablace = bez architecture layer v~\#1; Implementation ablace
= bez pre-konfigurovaného toolingu. R5 (bare minimum) zůstává jako
kumulativní benchmark.
\end{raw}

\textbf{Katalog ablatable komponent (brainstorm):}

Následující seznam identifikuje komponenty, které lze jednotlivě odebrat
a~měřit jejich dopad. Vychází z~empirických ablačních studií v~literatuře
-- SWE-agent \cite{yang2024sweagent} abluje ACI komponenty,
CCA \cite{wang2025cca} abluje scaffolding features,
Anthropic \cite{anthropic2025harness} identifikuje failure-specific komponenty.

\textit{Specifikace (BriefingScript):}
\begin{itemize}
    \item Architecture vrstva (types, invariants, behavioral model) \cite{specine2025}
    \item Doménový glossary \cite{domaincodegen2024}
    \item Detail acceptance criteria (konkrétní hodnoty vs. obecný popis) \cite{ticoder2024}
    \item Behavioral model / state diagram \cite{sommerville2016}
\end{itemize}

\textit{Instrukce (LoopScript + MentorScript):}
\begin{itemize}
    \item \texttt{agents.md} jako celek \cite{hassan2025sase}
    \item TDD instrukce (testy před implementací) \cite{mathews2024}
    \item Git workflow konvence (branching, commits, PR)
    \item Code quality standardy (strict mode, no-any)
    \item Code review instrukce
\end{itemize}

\textit{Prostředí (Agent Execution Environment):}
\begin{itemize}
    \item CI pipeline (automatická kontrola)
    \item Pre-konfigurovaný tooling (tsconfig, eslint, vitest, stryker)
    \item Project structure (adresářová struktura, package.json)
\end{itemize}

\textit{Orchestrace a~session management:}
\begin{itemize}
    \item Session-per-issue granularita -- empiricky validovaná jako optimální
          scope \cite{spotify2025context}; agenti selhávají při feature-level
          scope \cite{featurebench2026}
    \item Issue-based task decomposition -- dekompozice úkolů do sub-issues
          s~dependencies; task decomposition jako context management
          strategie \cite{chainofagents2024}
    \item Sub-agent delegace -- orchestrátor deleguje issues na sub-agenty
          s~čistým kontextem; hierarchická orchestrace dosahuje SOTA
          \cite{agentorchestra2025}; trust calibration mezi delegátorem
          a~delegátem \cite{googledelegation2026}
    \item Role separation -- oddělení analytických a~exekučních rolí.
          MASAI \cite{masai2024} definuje 5~specializovaných sub-agentů
          se structured artifact passing (ne konverzací); AgentCoder
          \cite{agentcoder2024} empiricky validuje separaci test designu
          od implementace (96,3\,\% HumanEval); Agyn \cite{agyn2026}
          replikuje inženýrský tým s~dedikovanými rolemi a~dosahuje
          72,2\,\% SWE-bench~500. Studie ukazují, že organizační design
          je stejně důležitý jako schopnosti modelu.
    \item Single vs. multi-agent trade-off -- přínos multi-agent architektury
          klesá s~rostoucí schopností modelů \cite{singleormulti2025};
          hybridní přístup (multi-agent jen tam kde je potřeba) přináší
          +1--12\,\% accuracy při --88\,\% cost. Multi-agent systémy
          mají 14~identifikovaných failure modes ve 3~kategoriích
          (system design, inter-agent misalignment, task verification)
          \cite{mast2025}. Minimální efektivní rozdělení = separace
          testování od implementace.
    \item Meta-prompting -- místo explicitních instrukcí agent dostane
          meta-instrukci navrhnout si vlastní workflow; task-agnostic
          scaffolding pattern \cite{suzgun2024metaprompt}; automatizovaný
          design agentních systémů \cite{hu2024adas}
    \item Completion verification loop -- mechanismus zajišťující, že agent
          dokončí celý projekt, ne jen jeden krok. Tři varianty:
          (1)~Ralph Loop -- vnější cyklus s~čistým kontextem per iteraci,
          stav přes soubory/issues, implementován v~Claude Code i~OpenCode;
          (2)~session.idle hook -- plugin uvnitř agentního frameworku,
          při ukončení agenta ověří podmínku dokončení (např. počet
          otevřených issues přes GitHub API) a~pokud není splněna,
          re-injektuje prompt s~aktuálním stavem; počet restartů slouží
          jako metrika autonomie agenta;
          (3)~instruktážní -- agent dostane instrukci ``neskončí dokud
          všechny issues nejsou uzavřeny'', bez vnější kontroly.
    \item Retry strategie -- single-shot vs. fresh-context retry vs.
          child fix issues; scaffolding a~retry jsou nezávisle hodnotné
          a~multiplikativní \cite{neurosymbolic2025}
    \item Strukturovaná dokumentace v~issues -- záznam pro následující
          agent session (co bylo zkušeno, co selhalo, aktuální stav)
          \cite{anthropic2025harness}. Každá inter-agent zpráva je potenciální
          failure point; chain-style error propagation kaskáduje malé nepřesnosti
          do systémových selhání \cite{agentask2025}. Tři validované mechanismy
          předávání stavu: (1)~progress file + git commity \cite{anthropic2025harness},
          (2)~GitHub issues jako komunikační kanál (issue $\to$ PR $\to$ review),
          (3)~structured artifact passing s~formální output specifikací
          \cite{masai2024}
\end{itemize}

\textit{Context window management:}
\begin{itemize}
    \item Context window degraduje s~délkou, ne jen ``dojde'' --
          coding performance klesá z~29\,\% na 3\,\% při long-context
          úlohách \cite{longcodebench2025}; U-shaped křivka, informace
          uprostřed se ztrácí \cite{liu2024lost}; nerovnoměrná degradace
          napříč 18 modely \cite{contextrot2025}
    \item Observation masking (skrytí detailů tool outputu) je stejně
          efektivní jako LLM summarization za poloviční cenu
          \cite{jetbrains2025complexity}
\end{itemize}

Tento katalog slouží jako zásobník pro výběr experimentálních běhů.
Ne všechny komponenty budou ablované -- výběr závisí na dostupném scope
a~síle literární opory pro konkrétní hypotézu.

\textit{Ablace vs. substituce:}
Primární experimentální manipulací je čistá ablace -- odebrání komponenty bez
náhrady. Pokud agent při absenci scaffoldingu spontánně vytvoří vlastní workflow
(meta-prompting chování), jedná se o~pozorování, ne o~experimentální manipulaci.
Explicitní substituce (nahrazení scaffoldingu meta-instrukcí ``navrhni si vlastní
postup'') je možným rozšířením katalogu, ale není součástí primární sady běhů.
Důvod: čistá ablace izoluje dopad absence konkrétní komponenty; substituce měří
rozdíl mezi dvěma přístupy, což ztěžuje kauzální interpretaci.

\textbf{Multi-issue gap:}

Stávající benchmarky ukazují výrazný propad výkonu agentů při přechodu
od single-issue k~multi-issue úlohám: SWE-EVO \cite{sweevo2025} reportuje
pouze 21\,\% úspěšnost na evolučních úlohách (průměrně 21~souborů) oproti
65\,\% na single-issue SWE-Bench; FeatureBench \cite{featurebench2026} měří
11\,\% na feature-level úlohách; ACE-Bench \cite{acebench2025} 7,5\,\%
na end-to-end feature development. Tato case study (5~issues, celý dunning
system) cílí přesně do tohoto rozsahu, kde scaffolding může přinést
měřitelný rozdíl.

\textbf{Odvozování testů z acceptance criteria:}

Acceptance criteria ve formátu Given/When/Then (BDD) s konkrétními hodnotami
jsou přímo mapovatelná na unit testy. Schopnost agenta korektně odvodit test suite
z~acceptance criteria je měřitelná dimenze experimentu -- odpovídá zjištění
TiCoder \cite{ticoder2024}, kde formalizace záměru přes testy vedla k~45.97\%
zlepšení Pass@1.

\textbf{Strategie testování a mutation testing:}

Klíčovým rizikem LLM-generovaných testů je tzv. \textbf{test oracle problem}
\cite{mathews2024}. Mathews et al. ukázali, že nástroje pro automatické generování
testů (CoverAgent, CoverUp) systematicky filtrují failing testy a~ponechávají
pouze passing -- výsledkem je, že až 68,1\,\% vygenerovaných test suites
\textbf{validuje chybné chování} místo jeho odhalení. Příčinou je, že expected
values jsou odvozeny z~pozorování kódu, ne ze specifikace.

Chen et al. \cite{rethinking2025} empiricky potvrdili na 500 úlohách SWE-bench,
že agent-generované testy slouží primárně jako \textbf{observační feedback}
(value-revealing prints), ne jako validační nástroj -- 83,2\,\% úloh má stejný
výsledek bez ohledu na to, zda agent testy píše. Relační a~boundary kontroly
(nejcennější pro detekci chyb) tvoří pouze 3--8\,\% assertions.

Obrana proti těmto anti-patterns:
\begin{itemize}
    \item \textbf{TDD ze specifikace} -- expected values vycházejí z~acceptance
          criteria, ne z~pozorování kódu. Agent píše testy \textbf{před}
          implementací (red $\rightarrow$ green $\rightarrow$ refactor).
    \item \textbf{Failing test = opravit kód, ne test} -- zabraňuje selection
          biasu, kde se zahazují testy odhalující chyby.
    \item \textbf{Mutation testing jako metrika kvality testů} -- strukturální
          coverage (branch, statement) je nutná, ale nedostatečná podmínka.
          Papadakis et al. \cite{papadakis2019} v~přehledové studii ukázali, že
          mutation score je silnější prediktor detekce reálných chyb než
          strukturální coverage -- 36\,\% chyb je odhalitelných pouze mutation
          testingem. Harman et al. \cite{meta2025} potvrdili v~produkčním
          nasazení na Meta (10\,795 tříd), že 70\,\% mutantů zůstává
          neodhalených i~při plném coverage.
\end{itemize}

Pro referenční implementaci volíme Stryker (mutation testing framework pro
TypeScript) jako objektivní metriku kvality testů agenta. Mutation score měří,
jak dobře testy detekují simulované chyby -- na rozdíl od coverage, která měří
pouze dosažitelnost kódu.

\textbf{Doplňková dimenze -- formát/reprezentace specifikace:}

Nezávisle na úrovni detailu ovlivňuje kvalitu výstupu i~způsob strukturování
informace. Volba kombinovaného formátu (structured text + tabulky + behavioral
model) vychází z~teorie vizuálních notací \cite{moody2009} a~výzkumu informační
hustoty diagramů \cite{larkin1987}. Systematické porovnání formátů je navrženo
jako future work.
\end{raw}

\section{Analýza}

\begin{raw}
[RAW]
Tradiční přístupy k měření kvality SW (podpora pro volbu dimenzí):

\textbf{Sommerville (Ch. 24, s. 705--728):}
\begin{itemize}
    \item Rozlišuje \textbf{control metrics} (procesní -- sledují proces vývoje) vs. \textbf{predictor metrics} (produktové -- měří vlastnosti kódu/dokumentů)
    \item Vztah proces-produkt u SW není přímočarý jako ve výrobě -- SW je designován, ne vyráběn, vliv individuálních dovedností je velký (s. 706)
    \item Produktové metriky (LOC, cyklomatická složitost) nemají jasný a konzistentní vztah ke kvalitativním atributům (s. 721)
    \item → Naše dimenze Functional Quality = predictor metrics, Compliance = control metrics
\end{itemize}

\textbf{McConnell -- Code Complete (Ch. 28, s. 715, Table 28-2):}
\begin{itemize}
    \item Kategorie měření: Size (LOC, třídy, komentáře) a Overall Quality (počet defektů, defekty/KLOC, mean time between failures)
    \item Praktický pohled -- co se dá reálně měřit v projektu
\end{itemize}

\textbf{SWEBOK v4 (Ch. 12, s. 248--256; Ch. 6, s. 176):}
\begin{itemize}
    \item Software Quality Measurement (s. 253) -- kvantifikace atributů pro rozhodování
    \item Míry údržby (s. 176): complexity, maintainability, testability, supportability, reliability
    \item Odkaz na ISO 25010 jako standard pro kvalitativní charakteristiky (s. 46, 256)
\end{itemize}

\textbf{SWE-bench (Appendix C.7, s. 28):}
\begin{itemize}
    \item Cyklomatická složitost (McCabe) a Halstead measures jako metriky pro hodnocení kódu v benchmarku
    \item Příklad jak existující benchmarky měří kvalitu kódu agentů -- ale jen funkční/strukturální, ne procesní
\end{itemize}

\textbf{Jin et al. 2024 -- LLM Agents SWE Survey (Table VII, s. 21):}
\begin{itemize}
    \item Přehled evaluačních metrik: Accuracy, Pass@k, Task Completion Time, Task Success, Execution Accuracy, Win-Rate
    \item Většina existující literatury měří hlavně funkční kvalitu výstupu
    \item → Naše dimenze Compliance a Alignment jsou méně pokryté v literatuře -- vlastní přínos
\end{itemize}
\end{raw}

\begin{raw}
Hodnocení výstupů agentů probíhá ve čtyřech dimenzích: funkční kvalita, procesní kvalita, efektivita a alignment.

\subsection{Functional Quality (Funkční kvalita)}

Měření funkčních vlastností výstupu dle ISO 25010:

\textbf{Completeness (Úplnost):}
\begin{itemize}
    \item Míra pokrytí požadované funkcionality
    \item Měření: Checklist požadavků ze specifikace → procento implementovaných
\end{itemize}

\textbf{Correctness (Správnost):}
\begin{itemize}
    \item Správnost implementace - funguje to jak má?
    \item Měření: Spuštění referenčních testů na kód agenta (pass rate)
    \item Kvalita testů agenta: Mutation testing (Stryker) - mutation score určuje jak dobře testy detekují chyby
\end{itemize}

\subsection{Compliance (Procesní kvalita)}

Dodržování softwarově-inženýrských praktik:

\textbf{Workflow:}
\begin{itemize}
    \item Dodržení flow: issues → branch → commits → PR
    \item Měření: Automatická kontrola git historie a GitHub artefaktů
\end{itemize}

\textbf{Conventions (Konvence):}
\begin{itemize}
    \item Kvalita commit messages (formát, atomicita, srozumitelnost)
    \item Kvalita issues (popis, acceptance criteria)
    \item Kvalita dokumentace a PR description
    \item Měření: LLM-as-a-judge s definovaným rubrikem \cite{llmjudge2024}
\end{itemize}

\textbf{Transparency (Transparentnost):}
\begin{itemize}
    \item Vysvětluje agent svá rozhodnutí?
    \item Dokumentuje postup a důvody?
    \item Měření: LLM-as-a-judge + manuální review
\end{itemize}

\subsection{Efficiency (Efektivita)}

Náklady na dosažení výsledku:

\begin{itemize}
    \item \textbf{Token usage} - spotřeba tokenů (náklady na API)
    \item \textbf{Iterations} - počet pokusů a oprav potřebných k dokončení
    \item \textbf{Time} - celkový čas do dokončení
    \item \textbf{Human intervention} - míra nutných lidských zásahů a korekcí
\end{itemize}

Měření: Logování z agenta a konverzačních sessions.

\subsection{Metody měření}

Kombinace tří přístupů:
\begin{itemize}
    \item \textbf{Automatické} - testy, mutation testing, git log analýza, token counting
    \item \textbf{LLM-as-a-judge} - hodnocení subjektivních aspektů (kvalita commit messages, dokumentace) pomocí LLM s definovaným rubrikem
    \item \textbf{Manuální review} - kvalitativní zhodnocení celku autorem
\end{itemize}

LLM-as-a-judge přístup využívá strukturované hodnocení kde LLM dostane kritéria a škálu, a konzistentně hodnotí všechny běhy. Validace tohoto přístupu probíhá porovnáním s manuálním hodnocením na vzorku \cite{llmjudge2024}.

\subsection{Alignment (Soulad se záměrem)}

Alignment měří, zda agent pochopil skutečný záměr zadání - ne jen doslovnou instrukci, ale co uživatel skutečně chtěl \cite{llmjudge2024}.

Agent může mít 100\% Correctness a Completeness, ale být misaligned - technicky splnil zadání, ale výsledek neodpovídá záměru.

\textbf{Co se hodnotí:}
\begin{itemize}
    \item \textbf{Over-engineering} - přidal agent funkcionalitu která nebyla požadována?
    \item \textbf{Under-delivering} - vynechal agent implicitní požadavky které byly zřejmé z kontextu?
    \item \textbf{Misinterpretation} - pochopil agent zadání špatně?
    \item \textbf{Scope adherence} - držel se agent vymezeného rozsahu?
\end{itemize}

\textbf{Měření:} Manuální review autorem + LLM-as-a-judge porovnávající zadání vs. skutečný výstup.

\subsection{Vyhodnocení}

Identifikace vzorů - která nastavení scaffoldingu vedla k lepším výsledkům v jednotlivých dimenzích. Porovnání trade-offs (např. vyšší kvalita vs. vyšší náklady).

\begin{raw}
[RAW]
\textbf{Rozšíření: meta-prompting jako navazující výzkum}

Ablační série testuje co se stane když explicitní scaffolding \textit{chybí}.
Přirozenou navazující otázkou je: co když ho nahradíme meta-instrukcí?
Místo ``použi tento workflow'' dostane agent ``navrhni si vlastní workflow
pro tento projekt''. Meta-prompting \cite{suzgun2024metaprompt} je
task-agnostic scaffolding pattern -- agent dostane strukturu myšlení,
ne konkrétní instrukce.

Tato substituce je metodologicky odlišná od ablace: neměříme absenci
scaffoldingu ale alternativní přístup k~němu. Výsledky ablační série
(které komponenty jsou klíčové) by určily kde má meta-prompting
největší potenciál -- tam kde explicitní instrukce prokazatelně pomáhají,
je zajímavé testovat jestli agent dokáže totéž odvodit sám.

Konkrétní varianty pro navazující studii:
\begin{itemize}
    \item Decomposition meta-prompt: ``rozděl projekt na logické části
          a~navrhni pořadí implementace'' místo human-prepared sub-issues
    \item Role meta-prompt: ``navrhni si jak strukturovat svou práci''
          místo orchestrator+builder separace
    \item Full meta-prompt baseline: veškerý scaffolding nahrazen
          meta-instrukcí; porovnání s~R0 ukáže celkový potenciál
          self-designed workflow
\end{itemize}

Toto rozšíření je mimo scope této BP -- vyžaduje samostatnou sérii běhů
a~hlubší analýzu interaction efektů. Je navrženo jako přímé pokračování.
\end{raw}
\end{raw}
