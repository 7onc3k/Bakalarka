\chapter{Metodika}
\label{kap:metodika}

%% ==========================================================================
%% 3.1 Výzkumný přístup
%% ==========================================================================

\section{Výzkumný přístup: návrhový výzkum (DSR)}
\label{sec:vyzkumny-pristup}

\begin{draft}
Hevner et al. \cite{hevner2004} rozlišují v~informatickém výzkumu dva přístupy. Behaviorální výzkum popisuje a~vysvětluje existující jevy --- například jak vývojáři pracují s~AI nástroji. Návrhový výzkum (Design Science Research, DSR) naopak navrhuje nové artefakty a~ověřuje, jestli řeší daný problém. Rozdíl je v~tom, co je výstupem: popis světa, nebo nástroj který ho mění.

Tato práce spadá do návrhového výzkumu. Artefaktem je sada instrukcí pro AI coding agenta. Instrukce se navrhnou, agent s~nimi projede vývojový úkol, z~výstupu (kód, testy, git log) se vyhodnotí co fungovalo a~co ne, a~instrukce se podle toho upraví. Hevner tento postup formalizuje jako build-evaluate cyklus --- iterativní smyčku návrhu a~vyhodnocení, která se opakuje dokud artefakt nedosáhne požadované kvality.

\subsubsection*{Dvě fáze výzkumu}

Výzkum má dvě fáze, které odpovídají DSR cyklu:

\begin{enumerate}
    \item \textbf{Pilotní iterace} --- opakovaně navrhujeme a~vyhodnocujeme instrukce. Každá iterace je jeden build-evaluate cyklus: spustíme agenta, analyzujeme výstup, diagnostikujeme problém, upravíme instrukce. Zaznamenáváme co se měnilo a~proč. Způsob měření popisuje sekce~\ref{sec:zpusob-mereni}, exit kritéria sekce~\ref{sec:exit-kriteria}.
    \item \textbf{Komparativní variace} --- z~fungujících instrukcí systematicky měníme jednotlivé komponenty a~měříme dopad na chování agenta. Změna může být odebrání (ablace --- funguje agent bez této instrukce?) nebo nahrazení alternativou (substituce --- funguje jiná formulace stejného záměru?). Ablace ukáže \textit{jestli} komponenta záleží, substituce ukáže \textit{proč} a~\textit{v~jaké formě}.
\end{enumerate}

\subsubsection*{Case study a~generalizace}

Výzkum probíhá formou case study na jednom projektu (systém upomínek faktur). Yin \cite{yin2018} rozlišuje dva typy generalizace: statistickou (ze vzorku na populaci) a~analytickou (z~případu na teorii). Case study neumožňuje říct ``tento scaffolding funguje vždy'' --- k~tomu by byl potřeba velký vzorek projektů. Umožňuje ale identifikovat principy a~mechanismy: \textit{proč} určité instrukce fungují a~jiné ne, a~za jakých podmínek.

Více experimentálních běhů na jednom projektu odpovídá tomu, co Yin nazývá embedded single-case design --- jeden případ (projekt) s~více vnořenými jednotkami analýzy (jednotlivé běhy). DSR poskytuje celkový rámec (jak iterovat artefakt), case study poskytuje kontext (reálný projekt s~deterministickou logikou a~ověřitelnými výstupy).
\end{draft}

%% ==========================================================================
%% 3.2 Výběr projektu
%% ==========================================================================

\section{Výběr projektu pro case study}

\begin{draft}
Experiment potřebuje projekt, na kterém lze spustit více běhů s~různým nastavením instrukcí a~objektivně měřit výsledky. Zvolený projekt musí splňovat:

\begin{itemize}
    \item \textbf{Hard logic} --- jasná business pravidla, ne subjektivní výstupy
    \item \textbf{Jasné invarianty} --- deterministické chování, ověřitelná správnost
    \item \textbf{Testovatelné} --- kvalitu výstupu lze měřit objektivně (testy, mutation score)
    \item \textbf{Přiměřená velikost} --- menší projekt umožňuje více experimentálních běhů
    \item \textbf{Reálný use case} --- prakticky využitelné, ne umělý příklad
\end{itemize}
\end{draft}

\subsection*{Systém upomínek faktur}

\begin{draft}
Systém pro automatické odesílání připomínek k~nezaplaceným fakturám. Obsahuje stavový automat pro sledování stavu faktury (nová, po splatnosti, upomínaná, eskalovaná), časové výpočty (pracovní dny, ochranné lhůty), pravidla pro eskalaci a~plánování odesílání upomínek.
\end{draft}

%% ==========================================================================
%% 3.3 Cíle artefaktu
%% ==========================================================================

\section{Cíle artefaktu}
\label{sec:cile-artefaktu}

\begin{draft}
[DRAFT] V~terminologii DSR \cite{hevner2004} je artefaktem sada instrukcí
pro AI coding agenta (\texttt{AGENTS.md}). Cíle artefaktu definují
co má artefakt způsobit --- z~nich se odvíjí volba metrik
(sekce~\ref{sec:zpusob-mereni}) i~exit kritéria pilotní fáze
(sekce~\ref{sec:exit-kriteria}).

\textbf{Procesní cíle} --- agent dodržuje strukturovaný vývojový proces:
\begin{enumerate}
    \item \textbf{Spec-first} --- agent odvozuje veškerou práci ze specifikace,
          ne z~trénovacích dat nebo předpokladů
    \item \textbf{Strukturovaný workflow} --- agent decomponuje, organizuje
          a~trackuje práci systematicky (issues $\to$ branches $\to$ PRs)
    \item \textbf{TDD} --- test-first vývojový cyklus s~průběžnými commity
    \item \textbf{Transparentní proces} --- procesní artefakty dokumentují
          co a~proč (commit messages, issue descriptions, PR descriptions)
\end{enumerate}

\textbf{Produktové cíle} --- agent vyprodukuje kvalitní software:
\begin{enumerate}
    \item Funkční korektnost --- implementace splňuje acceptance criteria
    \item Kompatibilní API --- dodržuje definovaný kontrakt
    \item Kvalitní testy --- testy skutečně detekují chyby
    \item Kvalitní kód --- čitelný, idiomatický, přiměřeně komplexní
\end{enumerate}

\textbf{Efektivita} --- agent pracuje v~rozumných mezích zdrojů
(tokeny, čas, autonomie).

Sekce~\ref{sec:zpusob-mereni} definuje metriky které operacionalizují
každý z~těchto cílů. Sekce~\ref{sec:exit-kriteria} stanoví minimální
prahy pro přechod z~pilotní do komparativní fáze.
\end{draft}

%% ==========================================================================
%% 3.4 Způsob měření
%% ==========================================================================

\section{Způsob měření}
\label{sec:zpusob-mereni}

\begin{draft}
[DRAFT] Metriky experimentu jsou organizovány podle taxonomie Fenton
a~Biemana \cite{fenton2014}: \textbf{procesní metriky} (jak agent
pracuje), \textbf{produktové metriky} (co agent vyrobil)
a~\textbf{metriky efektivity} (za jakou cenu). Toto členění je
standardní v~softwarovém inženýrství a~umožňuje oddělit hodnocení
procesu od hodnocení výstupu.

Všechny metriky mají jednoznačný kód pro referenci v~tabulkách
a~across-run srovnáních.

Měření kombinuje tři přístupy: automatické skripty (git log, testy,
statická analýza), LLM-as-judge s~fixním rubrikem \cite{llmjudge2024},
a~manuální review transkriptu.

Primární zdroje dat:
\begin{itemize}
    \item \textbf{Git log + GitHub API} --- commity, branches, PRs, issues
    \item \textbf{Session transcript} (\texttt{opencode export}) --- kompletní
          sekvence tool calls a~rozhodnutí agenta
    \item \textbf{Zdrojový kód agenta} --- vstup pro testy a~statickou analýzu
\end{itemize}
\end{draft}

\subsection{Procesní metriky}
\label{sec:procesni-metriky}

\begin{draft}
[DRAFT] Procesní metriky \cite{fenton2014} měří \textit{jak} agent
pracuje. Místo jednotlivých kvantitativních metrik pro každý aspekt
procesu používáme dvě komplementární měření: binární compliance
checklist (P1) a~kvalitativní hodnocení procesních artefaktů (P2).

\begin{description}
    \item[P1 --- process compliance.] Binární checklist pozorovatelný
        z~git logu a~GitHub API. Každá položka je splněna (1) nebo
        nesplněna (0), score = podíl splněných.
        \begin{enumerate}
            \item Vytvořil issues před kódem
                  (\texttt{gh issue list} timestamps vs. první kódový commit)
            \item Separátní branch per issue
                  (\texttt{git branch~-a} vs. issue count)
            \item Test commit před implementačním commitem
                  (\texttt{git log} --- pořadí commitů)
            \item PRs linkované na issues
                  (\texttt{gh pr list} --- ``Closes \#N'')
            \item Nemodifikoval existující test assertions
                  (\texttt{git diff} na test souborech)
            \item Typecheck prochází před PR
                  (CI výstup nebo commit historie)
        \end{enumerate}
        Všechny položky jsou automaticky extrahovatelné jedním skriptem.
        Žádná subjektivita --- binární výsledek, reprodukovatelný.
    \item[P2 --- kvalita procesních artefaktů.]
        Commit messages (popisnost, atomicita, konvenční prefix),
        issue descriptions (scope, AC, kontext),
        PR descriptions (co a~proč, odkaz na issue).
        Hodnocení LLM-as-judge (sekce~\ref{sec:llm-as-judge}).
\end{description}
\end{draft}

\subsection{Produktové metriky}
\label{sec:produktove-metriky}

\begin{draft}
[DRAFT] Produktové metriky (kód Q --- quality) \cite{fenton2014} měří
\textit{co} agent vyrobil --- funguje to? Jsou testy kvalitní? Je kód
udržovatelný?

\subsubsection*{Funkční korektnost (Q1--Q2)}

\begin{description}
    \item[Q1 --- referenční test pass rate.] 40 behavioral testů
        odvozených z~acceptance criteria metodou TDD ze specifikace
        \cite{mathews2024} (viz sekce~\ref{sec:referencni-implementace}).
        Testují chování přes veřejné API --- nezávislé na interní
        struktuře agentovy implementace. Odpovídá přístupu SWE-bench
        \cite{swebench2024}. Pass rate = projité testy / 40. Binární
        varianta (pass all / fail) odpovídá metrice \% Resolved
        v~SWE-bench.
    \item[Q2 --- API contract match.] \texttt{tsc} import + type-check
        referenčních typů proti agentovu kódu. Ověřuje kompatibilitu
        s~definovaným rozhraním.
\end{description}

\subsubsection*{Kvalita testů (Q3--Q4)}

\begin{description}
    \item[Q3 --- mutation score (Stryker).] Podíl zabitých mutantů ---
        měří jestli agentovy testy skutečně detekují chyby, nebo jsou
        tautologické. Silnější prediktor než strukturální coverage ---
        36\,\% chyb odhalitelných pouze mutation testingem
        \cite{papadakis2019}. Harman et al. \cite{meta2025} potvrdili
        v~produkčním nasazení na Meta, že 70\,\% mutantů zůstává
        neodhalených i~při plném coverage.
    \item[Q4 --- AC coverage.] Kolik z~24 acceptance criteria má
        odpovídající test. Měří úplnost pokrytí požadavků, ne jen
        korektnost implementace. Mapování test $\to$ AC manuální
        nebo LLM-as-judge.
\end{description}

\textbf{Referenční hodnoty} (z~referenční implementace):
\begin{itemize}
    \item Mutation score: $\geq$ 74,3\,\%
    \item AC coverage: 24/24
\end{itemize}

\subsubsection*{Kvalita kódu (Q5--Q8)}

\textbf{Automatické metriky:}
\begin{description}
    \item[Q5 --- lint warnings.] \texttt{eslint --format json}, počet
        varování a~chyb.
    \item[Q6 --- typecheck errors.] \texttt{tsc --noEmit}, počet chyb.
        Strict mode compliance --- počet \texttt{any} v~kódu.
    \item[Q7 --- složitost kódu.] Cyklomatická složitost per funkce
        (ESLint \texttt{complexity} rule), maximální délka funkce.
        SWE-bench používá McCabe jako jednu z~metrik \cite{swebench2024}.
\end{description}

\textbf{Q8 --- kvalita kódu (LLM-as-judge):}
Naming conventions, separation of concerns, idiomatický TypeScript,
kvalita dokumentace, zbytečná komplexita. Hodnocení LLM-as-judge
(sekce~\ref{sec:llm-as-judge}).
\end{draft}

\subsection{Metriky efektivity}
\label{sec:metriky-efektivity}

\begin{draft}
[DRAFT] Metriky efektivity měří zdroje spotřebované při vývoji ---
standardní dimenze v~agent benchmarcích (SWE-bench, AgentBench).

\begin{description}
    \item[E1 --- tokeny.] Input/output tokeny: \texttt{opencode export}
          $\to$ JSON parsing.
    \item[E2 --- trvání.] Wall-clock time v~minutách (session timestamps).
    \item[E3 --- kompletní dokončení.] Crash / no crash + důvod.
          Počet restartů auto-continue pluginu jako proxy pro autonomii.
\end{description}

S~N=1 per run jsou tyto metriky deskriptivní (porovnání napříč běhy),
ne inferenční (žádné p-hodnoty).
\end{draft}

\subsection{Hodnocení LLM-as-judge}
\label{sec:llm-as-judge}

\begin{draft}
[DRAFT] Metriky P2 (procesní artefakty) a~Q8 (kvalita kódu) hodnotí
aspekty které nelze měřit deterministicky. Pro obě používáme metodu
LLM-as-judge \cite{zheng2023mtbench} --- LLM hodnotí výstup agenta
podle fixního rubriku.

\subsubsection*{Self-preference bias a~volba modelu}

Panickssery et al. \cite{panickssery2024} prokázali kauzální vztah
mezi schopností LLM rozpoznat vlastní výstupy a~preferencí těchto
výstupů při hodnocení. Model který generoval hodnocený text má
tendenci ho hodnotit výše --- mechanismem je nižší perplexita
vlastních výstupů. Zheng et al. \cite{zheng2023mtbench} identifikovali
self-enhancement bias jako jeden ze tří systematických biasů
LLM-as-judge (spolu s~position bias a~verbosity bias).

Z~toho plyne: model hodnotitele musí být z~\textbf{jiné rodiny}
než model který generoval hodnocený kód \cite{verga2024poll}.
V~našem experimentu generuje kód minimax-m2.5-free (rodina MiniMax).
Jako hodnotitele volíme GLM-5 (rodina Zhipu AI) --- odlišná
architektura i~trénovací data, čímž je eliminován perplexitní
mechanismus self-preference.

\subsubsection*{Design hodnocení}

\begin{itemize}
    \item \textbf{Model:} GLM-5 (Zhipu AI) přes OpenCode
    \item \textbf{Škála:} 1--3 per dimenze (poor / acceptable / good) ---
          nižší granularita je spolehlivější při malém N
    \item \textbf{Rubric:} fixní prompt s~definicí per úroveň
          a~příklady, totožný pro všechny běhy (příloha~X)
    \item \textbf{Jeden prompt per metriku} --- P2 hodnotí tři dimenze
          procesních artefaktů, Q8 čtyři dimenze kvality kódu, každý
          s~vlastním rubrikem
    \item \textbf{Validace:} autor ohodnotí podmnožinu běhů manuálně,
          shoda s~LLM-as-judge vyjádřena Cohenovým $\kappa$
\end{itemize}

\subsubsection*{Hodnocené dimenze}

\textbf{P2 --- procesní artefakty:}
commit messages (popisnost, atomicita, konvenční prefix),
issue descriptions (scope, AC, kontext),
PR descriptions (co a~proč, odkaz na issue).

\textbf{Q8 --- kvalita kódu:}
naming conventions, separation of concerns, idiomatický TypeScript,
kvalita dokumentace, zbytečná komplexita (inverzně).

Rozdíl: P2 měří jak agent \textit{komunikuje} (procesní dimenze),
Q8 měří co agent \textit{vyrobil} (produktová dimenze).

[DRAFT] Rubriky jsou definovány v~experimentální infrastruktuře
(\texttt{infra/judge/p2-process-artifacts.md}
a~\texttt{infra/judge/q8-code-quality.md}). Kompletní znění
rubriků včetně příkladů bude v~příloze.
\end{draft}

\subsection{Referenční test suite}
\label{sec:referencni-implementace}

\begin{draft}
[DRAFT] Metrika Q1 (funkční korektnost) vyžaduje sadu referenčních testů
nezávislou na agentově implementaci. Tato test suite je měřicí nástroj
--- analogicky k~tomu jak LLM-as-judge je nástrojem pro P2 a~Q8.

\textbf{Metoda: TDD z acceptance criteria.}
Postup odpovídá spec-first TDD \cite{mathews2024}: nejprve se napíší
behavioral testy přímo z~AC ve formátu Given/When/Then --- testy jsou
zpočátku červené (implementace neexistuje). Následně se implementuje
referenční řešení tak, aby testy postupně zelenaly. Tato sekvence
zajišťuje, že expected values pochází ze specifikace, nikoliv
z~pozorování kódu (test oracle problem \cite{mathews2024}).

\textbf{Behavioral testy, ne unit testy.}
Referenční testy testují chování systému přes veřejné API (vstupy
a~výstupy definované specifikací), nikoliv interní implementaci.
Tento přístup odpovídá black-box testování funkčních požadavků
\cite{swebok2024} a~metodice SWE-bench \cite{swebench2024}.
Výhodou je přenositelnost: stejné testy lze spustit na implementaci
libovolného agentního běhu bez znalosti jeho interní struktury.

\textbf{Vedlejší přínos: validace specifikace.}
Implementace referenčního řešení odhaluje nejednoznačnosti a~mezery
v~acceptance criteria dříve než experimentální běhy.

Konkrétní výsledky referenční implementace (počet testů, mutation
score, pokrytí AC) jsou v~sekci~\ref{sec:pilotni-vysledky}.
\end{draft}

%% ==========================================================================
%% 3.5 Experimentální design
%% ==========================================================================

\section{Experimentální design}
\label{sec:experimentalni-design}

\subsection{Fixní proměnné}

\begin{draft}
[DRAFT] Všechny běhy sdílejí stejný setup --- jedinou proměnnou je obsah
\texttt{AGENTS.md}.

\begin{description}
    \item[Prázdné GitHub repo] obsahuje pouze \texttt{AGENTS.md},
          konfiguraci agenta a~auto-continue plugin. Žádný existující
          kód, testy ani \texttt{package.json} --- agent si musí
          zvolit project structure, inicializovat projekt a~nastavit
          tooling sám. Tím se testuje schopnost agenta pracovat
          od nuly podle specifikace.
    \item[Specifikace v~GitHub Issue \#1] --- 24 acceptance criteria,
          API contract (TypeScript typy a~signatury), doménový glossary
          a~out of scope. Jediný vstup který agent dostane kromě
          instrukcí.
    \item[Auto-continue plugin] --- \texttt{session.idle} hook který
          detekuje kdy se agent zastaví (tool approval, idle) a~automaticky
          ho restartuje. Počítadlo restartů a~kontrola otevřených issues
          zajišťují, že experiment běží bez manuálního zásahu dokud agent
          neuzavře všechny issues nebo nedosáhne limitu restartů.
          Metrika E3 (kompletní dokončení) z~toho čerpá.
    \item[Model: minimax-m2.5-free] přes OpenCode. Volba na základě
          dostupnosti (free tier, bez rate limitů) a~rychlosti inference.
          Jiná modelová rodina než judge (GLM-5, Zhipu AI) ---
          eliminuje self-preference bias při hodnocení
          (sekce~\ref{sec:llm-as-judge}).
    \item[System prompt \texttt{build.md}] (\texttt{mode: primary})
          nahrazuje defaultní system prompt agenta. Obsahuje pouze
          kódové konvence (TypeScript strict, ESM) --- žádné procesní
          instrukce, ty jsou výhradně v~\texttt{AGENTS.md}.
          Oddělení zajišťuje, že procesní chování agenta je řízeno
          jedinou proměnnou experimentu.
\end{description}
\end{draft}

\subsection{Pilotní iterace}
\label{sec:pilotni-iterace}

\begin{draft}
[DRAFT] DSR build-evaluate cyklus aplikovaný na instrukce. Pilot končí
když agent splní exit kritéria (sekce~\ref{sec:exit-kriteria}).

\textbf{Každá iterace:}
\begin{enumerate}
    \item \textbf{Run} --- skript \texttt{new-run.sh} vytvoří prázdné
          GitHub repo, zkopíruje aktuální \texttt{AGENTS.md}, vytvoří
          Issue~\#1 se specifikací a~spustí agenta. Session transcript
          se exportuje po dokončení.
    \item \textbf{Measure} --- skript \texttt{analyze-run.sh} extrahuje
          metriky ze tří zdrojů:
          \begin{itemize}
              \item \textit{Git log + GitHub API} $\to$ P1 compliance
                    checklist (6 binárních položek)
              \item \textit{Testy + statická analýza} $\to$ Q1 (referenční
                    testy), Q2 (typecheck), Q3 (mutation score),
                    Q5 (lint), Q6 (typecheck errors), Q7 (složitost)
              \item \textit{Session metadata} $\to$ E1--E3
                    (tokeny, trvání, restart count)
          \end{itemize}
          Skript \texttt{judge.sh} spouští LLM-as-judge hodnocení
          pro P2 (procesní artefakty) a~Q8 (kvalita kódu).
    \item \textbf{Diagnose} --- identifikovat kde a~proč agent nedodržel
          očekávané chování. Diagnostický rámec:
          \begin{itemize}
              \item \textit{Component analysis} \cite{mao2025fse} --- která
                    z~komponent instrukcí (directive, context, workflow,
                    constraints) selhala?
              \item \textit{Script balance} \cite{hassan2025sase} ---
                    dominuje LoopScript na úkor BriefingScript/MentorScript?
              \item \textit{Content effectiveness} \cite{lulla2026} ---
                    přidává řádek hodnotu, nebo jen tokeny?
              \item \textit{Prompt sensitivity} \cite{razavi2025, breunig2025} ---
                    ignoruje agent instrukci opakovaně? $\rightarrow$
                    přestrukturovat, ne opakovat
          \end{itemize}
    \item \textbf{Fix} --- upravit \texttt{AGENTS.md} s~odkazem na diagnózu
          a~literaturu. Každá změna se zaznamená do CHANGELOG s~citací.
    \item \textbf{Repeat} --- dokud agent konzistentně splní exit kritéria
\end{enumerate}

\textbf{Výstup každé iterace:}
\begin{itemize}
    \item Aktualizovaný \texttt{AGENTS.md} (diff oproti předchozí verzi)
    \item CHANGELOG záznam: co se změnilo, proč, jaká evidence vedla ke~změně
    \item Kompletní P/Q/E metriky per run
    \item FINDINGS.md --- behavioral trace, diagnóza, návrh fixu
\end{itemize}

Konkrétní skripty a~jejich konfigurace jsou v~příloze~X.

\end{draft}

\subsection{Exit kritéria pilotní fáze}
\label{sec:exit-kriteria}

\begin{draft}
[DRAFT] Exit kritéria definují minimální prahy pro přechod z~pilotní
do komparativní fáze. Každé kritérium je odvozeno z~cíle artefaktu
(sekce~\ref{sec:cile-artefaktu}) a~operacionalizováno metrikou
ze sekce~\ref{sec:zpusob-mereni}.

Kritéria jsou rozdělena do dvou kategorií podle typu prahu.

\textbf{Deterministická kritéria} --- metriky s~přirozeným binárním
prahem (0 nebo all). Výsledek je jednoznačný, reprodukovatelný
a~nezávisí na úsudku hodnotitele.

\begin{tabular}{lllll}
\textbf{Cíl (sekce~\ref{sec:cile-artefaktu})}
    & \textbf{Metrika} & \textbf{Práh} & \textbf{Zdůvodnění} \\
\hline
Spec-first, workflow, TDD
    & P1 & 6/6 & binární checklist --- splněno nebo ne \\
Funkční korektnost
    & Q1 & 40/40 & SWE-bench binary (pass all) \\
API kompatibilita
    & Q2 & match & deterministické (\texttt{tsc}) \\
Pokrytí požadavků
    & Q4 & 24/24 & všechny AC mají test \\
Kvalitní kód (lint)
    & Q5 & 0 & CI gate standard \\
Kvalitní kód (typecheck)
    & Q6 & 0 & CI gate standard \\
\end{tabular}

\textbf{Minimální standard} --- metriky kde přirozený binární práh
neexistuje, ale lze stanovit odůvodněný minimální standard.

\begin{tabular}{lllll}
\textbf{Cíl (sekce~\ref{sec:cile-artefaktu})}
    & \textbf{Metrika} & \textbf{Práh} & \textbf{Zdůvodnění} \\
\hline
Transparentní proces
    & P2 & $\geq$ 2/3 & acceptable na škále LLM-as-judge \\
Kvalitní testy
    & Q3 & $\geq$ 70\,\% & industry baseline \cite{meta2025} \\
Kvalitní kód (složitost)
    & Q7 & $\leq$ 10 per fn & McCabe 1976, de facto standard \\
Kvalitní kód (design)
    & Q8 & $\geq$ 2/3 & acceptable na škále LLM-as-judge \\
\end{tabular}

\textbf{Záznam bez prahu} --- metriky efektivity (E1--E3) se
zaznamenávají pro across-run srovnání, ale nemají pass/fail
charakter. Slouží k~porovnání nákladů mezi iteracemi.

``Konzistentně'' = v~posledním běhu bez manuálního zásahu.
S~ohledem na scope práce (jeden projekt, jeden model) je
kritériem jeden úspěšný run --- ne opakovaná replikace.
\end{draft}

\subsection{Komparativní variace}

\begin{draft}
[DRAFT] Z~fungující sady instrukcí (výstup pilotní fáze) systematicky
měníme jednotlivé komponenty a~měříme dopad na chování agenta.

\textbf{Možné typy změn:}
\begin{description}
    \item[Ablace] --- odebrání komponenty. Měří \textit{nutnost}:
          funguje agent bez této instrukce?
    \item[Substituce] --- nahrazení komponenty alternativou. Měří
          \textit{efekt formulace}: která varianta produkuje lepší výsledky?
\end{description}

\textbf{Výběr variací.}
Konkrétní variace nelze specifikovat předem --- v~DSR pilotní fáze
\textit{generuje} hypotézy, komparativní fáze je testuje.
Výběr bude vycházet z~diagnostiky pilotních běhů pomocí rámců
z~sekce~\ref{sec:pilotni-iterace}:
\begin{itemize}
    \item Mao et al. \cite{mao2025fse} --- 7 komponent prompt šablony
          (role, directive, context, workflow, output, constraints,
          examples). Ablace: odebrat jednu komponentu a~měřit dopad.
    \item Hassan et al. \cite{hassan2025sase} --- balance
          BriefingScript (co a~proč) vs. LoopScript (jak) vs.
          MentorScript (co ne). Substituce: změnit poměr skriptů.
    \item Lulla et al. \cite{lulla2026} --- architektura a~konvence
          jsou efektivní, verbose workflow ne. Ablace: odebrat
          workflow detail a~ponechat jen cíle.
\end{itemize}

\textbf{Měření.}
Každá variace se měří identickými metrikami jako pilotní běhy
(P1--P2, Q1--Q8, E1--E3). Baseline pro srovnání je poslední
úspěšný pilotní run. Fixní proměnné
(sekce~\ref{sec:experimentalni-design}) zůstávají stejné ---
mění se pouze obsah \texttt{AGENTS.md}.

\textbf{Limitations.}
\begin{itemize}
    \item Prompt sensitivity \cite{razavi2025}: výsledky mohou záviset
          na konkrétní formulaci, ne jen na přítomnosti/absenci
          komponenty. S~N=1 per variaci nelze odlišit efekt formulace
          od šumu.
    \item Single-agent izolace: experiment předpokládá jednoho agenta
          pracujícího sám na čistém repozitáři. Nezohledňujeme
          scénáře s~externími příspěvky (code review, paralelní
          branches, merge conflicty z~vnějšku). Reálné nasazení
          typicky zahrnuje interakci s~lidskými vývojáři.
\end{itemize}
\end{draft}
